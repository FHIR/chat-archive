[
    {
        "content": "<p>Also for discussion today: <a href=\"#narrow/stream/179166-implementers/topic/Specs.20and.20Unicode\">https://chat.fhir.org/#narrow/stream/179166-implementers/topic/Specs.20and.20Unicode</a> if Grahame is able to join.</p>",
        "id": 267474905,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1641837282
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"358700\">@Guy Becker</span> <span class=\"user-mention\" data-user-id=\"197072\">@Paul Church</span> <span class=\"user-mention\" data-user-id=\"191407\">@Rick Geimer</span> <span class=\"user-mention\" data-user-id=\"191414\">@Dan Gottlieb</span>  -- We've made progress on <a href=\"http://jira.hl7.org/browse/FHIR-34475\">FHIR-34475</a> and agreed to add to this to spec,  with some adjustments:</p>\n<ul>\n<li>renamed to \"derivation-reference\"</li>\n<li>added a way to point to \"text documents\" (via DocumentReference, or Binary) as well as arbitrary other text content in a FHIR resource (e.g. Composition.section narrative)</li>\n</ul>\n<p>In the discussion, Paul mentioned some other use cases including 2D imaging annotations, more metadata about the ML model / algorithms, etc. I'd like to schedule  follow-up discussion to explore how we can enhance the model. Please thumbs-up here if you want to join that discussion.</p>",
        "id": 267499345,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1641848620
    },
    {
        "content": "<p>Think <span class=\"user-mention\" data-user-id=\"357010\">@Bin Mao</span> on the BCH NLP team will interested as well.</p>",
        "id": 267499605,
        "sender_full_name": "Dan Gottlieb",
        "timestamp": 1641848732
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191320\">@Lloyd McKenzie</span> in the resolution yesterday it looks like you changed context from <code>Resource, Element</code> to <code>Element</code>, but I don't think this was discussed. I don't <em>think</em> a context of <code>Element</code> allows for use at the resource level, does it? In which case we probably need to update to support both.</p>",
        "id": 267613300,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1641921043
    },
    {
        "content": "<p>There's an Element that represents the overall resource level in the snapshot.  However, if we need Resource too, that's fine.  Just note it in the comments.</p>",
        "id": 267645593,
        "sender_full_name": "Lloyd McKenzie",
        "timestamp": 1641935510
    },
    {
        "content": "<p>I don't quite follow your note about snapshot, but I think we did mean to write \"Resource, Element\". I added a comment to note this at <a href=\"https://jira.hl7.org/browse/FHIR-34475?focusedCommentId=195083&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-195083\">https://jira.hl7.org/browse/FHIR-34475?focusedCommentId=195083&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-195083</a> to this effect, thanks. (FYI @*<em>Guy Becker</em>.)</p>",
        "id": 267658076,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1641941487
    },
    {
        "content": "<p>OK, we've scheduled a call to accommodate everyone who responded with a \"thumbs up\" to my message above. I'll include details here in case others are interested in joiniong.</p>\n<p><strong>Call is  11a-12p CT on Jan 26</strong>: <a href=\"https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZDYzMzM0MjgtOGEwOS00ODdhLWE2NDktMGQ1ZjFiYjM2YTBm%40thread.v2/0?context=%7b%22Tid%22%3a%2272f988bf-86f1-41af-91ab-2d7cd011db47%22%2c%22Oid%22%3a%2299f26c89-9bc7-4381-abb4-95b0804b27f3%22%7d\">Microsoft Teams meeting</a></p>",
        "id": 267910991,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1642098739
    },
    {
        "content": "<p>Bump since the meeting starts in a few minutes, cheers!</p>",
        "id": 269445384,
        "sender_full_name": "Gino Canessa",
        "timestamp": 1643216591
    },
    {
        "content": "<p>Thanks to all who joined today's discussion! I've posted the recording at <a href=\"https://youtu.be/kPye_GjWrBQ\">https://youtu.be/kPye_GjWrBQ</a> (may take YouTube a few minutes to complete processing).</p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"kPye_GjWrBQ\" href=\"https://youtu.be/kPye_GjWrBQ\"><img src=\"https://uploads.zulipusercontent.net/ab3ab21868cf2cb23b9c6685d16dc2e1b4456ffd/68747470733a2f2f692e7974696d672e636f6d2f76692f6b5079655f476a577242512f64656661756c742e6a7067\"></a></div>",
        "id": 269472423,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1643227201
    },
    {
        "content": "<p>Call notes are <a href=\"https://hackmd.io/9qEVCNnZRQquPZKCrPS_8g?edit\">available here</a>.</p>",
        "id": 269472533,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1643227223
    },
    {
        "content": "<p>From our next steps on extensions: there's interest in communicating coordinates or bounding boxes for data derived from 2D images (e.g. OCR'd documents). Looking at real-world examples (e.g. <a href=\"https://cloud.google.com/vision/docs/reference/rest/v1p2beta1/images/annotate#BoundingPoly\">Google</a> and <a href=\"https://westus.dev.cognitive.microsoft.com/docs/services/computer-vision-v3-2/operations/5d986960601faab4bf452005\">Microsoft</a> and <a href=\"https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentTextDetection.html\">AWS</a>) there's quite a lot of capability in contemporary OCR toolchains -- e.g.</p>\n<ul>\n<li>hierarchical relationships between items like pages, paragraphs, and words</li>\n<li>orientation markers, adjustment rotations, and other pre-processing applied in order to \"rectify\" images</li>\n</ul>\n<p>My take here, though, is that we're not trying to develop a meta-model for full OCR capabilities. Rather, just want a pointer from FHIR data back to original source materials. As such, being able to draw a simple \"bounding box\" would be the rough \"2d image\" equivalent of our text-based \"offest and length\" annotations, and would be a good place to start. As such, I'd suggest adding to our existing extension a set of optional properties for use with 2D image sources:</p>\n<ul>\n<li><code>xmin</code>: <code>0..1 positiveInt</code></li>\n<li><code>xmax</code>: <code>0..1 positiveInt</code></li>\n<li><code>ymin</code>: <code>0..1 positiveInt</code></li>\n<li><code>ymax</code>: <code>0..1 positiveInt</code></li>\n</ul>\n<p>A constraint would ensure that these four properties were all present or absent -- something like <code>(xmin or xmax or ymin or ymax) implies (xmin and xmax and ymin and ymax)</code>.</p>",
        "id": 270091164,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1643657595
    },
    {
        "content": "<p>Should we take this forward, or is there an alternative design folks would like to suggest as an enhancement to <a href=\"https://build.fhir.org/extension-derivation-reference.html\">https://build.fhir.org/extension-derivation-reference.html</a> ?</p>",
        "id": 270091806,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1643657841
    }
]