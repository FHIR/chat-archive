[
    {
        "content": "<p>Welche FHIR-Server-Typen mit welcher Persistenz-Schicht verwendet ihr an den verschiedenen Standorten ? Im Moment verwenden wir HAPI mit einem Postgre-SQL-Datenbank dahinter. Die Postgre-Datenbank würde ich gerne zu einer MSSQL-Datenbank wechseln, da ich dann unseren zentral gehosteten MSSQL-Cluster verwenden könnte. Ich habe aus der HAPI-Doku jedoch noch nicht genau rauslesen können ob MSSQL wirklich untersützt wird.<br>\nWeiterhin würde mich intressieren was ihr an den verschiedenen Standorten an Mengen an Daten im Server habt ? Wenn ich alle Patienten mit z.B. allen 180 Mio Labormessungen und 210 Mio Bewegungen importieren würde, würde ich mich meinem aktuellen durchscnittlichen Durchsatz von maximal ca. 400 Resourcen/s im Import mindestens 12 Tage brauchen nur um diese beiden Datendomänen zu importieren. Geht das irgendwie irgendwo schneller ?</p>",
        "id": 269386775,
        "sender_full_name": "Georg Fette",
        "timestamp": 1643191798
    },
    {
        "content": "<p>Wir haben schon einiges durchprobiert, ohne ein FHIR-Server/DB-Kombination gefunden zu haben, die wirklich viel performanter ist und als Produkt befriedigt. Derzeit bauen wir eine Kombination von IBM FHIR-Server (in Kubernetes) + Postgres (auf Blech) auf. Entscheidend für die Performance einer Beladung mit großen Datenmengen wird aber eher sein, den Import über Parallelisierung und über die Verwendung von größeren Bundles zu beschleunigen oder besser gleich Bulk Import zu verwenden. Letzteres haben wir aber auch noch nicht ausprobieren können.</p>",
        "id": 269391518,
        "sender_full_name": "Detlef Kraska",
        "timestamp": 1643194505
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"196537\">Georg Fette</span> <a href=\"#narrow/stream/179307-german.2Fmi-initiative/topic/Server.20Typen.20und.20Datenmengen/near/269386775\">said</a>:</p>\n<blockquote>\n<p>Welche FHIR-Server-Typen mit welcher Persistenz-Schicht verwendet ihr an den verschiedenen Standorten ? Im Moment verwenden wir HAPI mit einem Postgre-SQL-Datenbank dahinter. Die Postgre-Datenbank würde ich gerne zu einer MSSQL-Datenbank wechseln, da ich dann unseren zentral gehosteten MSSQL-Cluster verwenden könnte. Ich habe aus der HAPI-Doku jedoch noch nicht genau rauslesen können ob MSSQL wirklich untersützt wird.<br>\nWeiterhin würde mich intressieren was ihr an den verschiedenen Standorten an Mengen an Daten im Server habt ? Wenn ich alle Patienten mit z.B. allen 180 Mio Labormessungen und 210 Mio Bewegungen importieren würde, würde ich mich meinem aktuellen durchscnittlichen Durchsatz von maximal ca. 400 Resourcen/s im Import mindestens 12 Tage brauchen nur um diese beiden Datendomänen zu importieren. Geht das irgendwie irgendwo schneller ?</p>\n</blockquote>\n<p>Du könntest die Resourcen direkt in die DB schreiben (gzipped Json), und danach einen re-index anstoßen.<br>\nSollte funktionieren (nicht selbst getestet).<br>\nMSSQL ist durch hapi supported.</p>",
        "id": 269396090,
        "sender_full_name": "Patrick Werner",
        "timestamp": 1643196762
    },
    {
        "content": "<p>Das mit dem direkt in die Datenbank schreiben haben wir mit dem VONK irgendwann schonmal ausprobiert, das ging wirklich sehr schnell. Das habe ich mit HAPI noch nicht ausprobiert. Hat jemand mit MSSQL und Hapi schonmal ein Setup ausprobiert ? In der JPA Server Dokumentation im Wiki gibt es einen Abschnitt MySQL und einen PostgreSQL aber keinen MSSQL. Muss man das einfach analog mit MSSQL-Parametern experimentieren ? Was wäre dann die driverClass ?</p>\n<p>Bulk Import ist mit HAPI scheinbar noch nicht implementiert. Unsere aktuell Performance bekommen wir auch nur mit Parallellisierung hin. Bei unserem aktuellen Hardware-Setup haben wir einen Sweetspot an maximalen Beladungs-Threads gefunden und könnten darüber hinaus höchstens noch mehr Hardware aufrüsten um besser zu werden. Ich hätte aber gerne noch mindestens den Faktor 3 raus um den Versuch zu machen alle Daten aller Patienten zu laden.</p>",
        "id": 269403828,
        "sender_full_name": "Georg Fette",
        "timestamp": 1643200837
    },
    {
        "content": "<p>man kann hier auch gut in der SMILE config spickeln: <a href=\"https://smilecdr.com/docs/database_administration/setting_up_sql_server_mssql.html#database-connection-properties\">https://smilecdr.com/docs/database_administration/setting_up_sql_server_mssql.html#database-connection-properties</a></p>",
        "id": 269403925,
        "sender_full_name": "Patrick Werner",
        "timestamp": 1643200901
    },
    {
        "content": "<p>sollten die Settings für MSSQL sein</p>",
        "id": 269403944,
        "sender_full_name": "Patrick Werner",
        "timestamp": 1643200912
    },
    {
        "content": "<p>ah, die SMILE-Konfiguration ist ein netter Tip</p>",
        "id": 269404023,
        "sender_full_name": "Georg Fette",
        "timestamp": 1643200940
    }
]