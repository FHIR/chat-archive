[
    {
        "content": "<p>Hello all,</p>\n<p>we are currently working on a research project for a clinic in which data should be transferred to a FHIR server.<br>\nAs we are relatively new to the FHIR community we started by setting up a HAPI FHIR server which went reasonably well with the existing information we could find. The data to be transformed is stored in a SQL database within the clinical information system.</p>\n<p>Now I'm trying to find ways to map and convert from the given relational data to FHIR resources. I started out by using Pentaho Kettle as an ETL tool, but I find it quite tedious, static, and not really fit for the task. I'm a bit confused about how little information I could find about this process and hit a bit of a dead-end. Maybe I'm looking for the wrong stuff. <br>\nSo I guess my question is, are there any best practices, tutorials, or experiences you could share on how to initially transform relational data into FHIR resources? Or is it all custom written implementations nobody wants to talk about?<br>\nI stumbled across <a href=\"https://github.com/arkhn/pyrog\">https://github.com/arkhn/pyrog</a> which looks quite promising but I'm not sure at which stage the development is.<br>\nI hope you can give me some advice or direction to look into.</p>\n<p>Thanks in advance</p>",
        "id": 217386276,
        "sender_full_name": "Florian Strieg",
        "timestamp": 1605869648
    },
    {
        "content": "<p>I expect the general belief is that there's nothing terribly FHIR-specific about this.  A layer like Hybernate to convert between your particular relational structure to the data model would commonly be used.  Exactly what the mapping looks like will vary widely - because legacy persistance models vary widely.  You might find some useful guidance on these two streams (though I can't promise it because I don't follow either of them :&gt;)  <a class=\"stream\" data-stream-id=\"179289\" href=\"/#narrow/stream/179289-storage-for-FHIR\">#storage for FHIR</a> <a class=\"stream\" data-stream-id=\"181579\" href=\"/#narrow/stream/181579-mapping-framework\">#mapping-framework</a></p>",
        "id": 217398474,
        "sender_full_name": "Lloyd McKenzie",
        "timestamp": 1605877918
    },
    {
        "content": "<p>So you need to transfer the data to a research database (you can’t keep it where it is already?)</p>\n<p>If you can keep it where it is, I’ve gone for a mix of hapi and hibernate. Here I’d coded the persistence layer.</p>\n<p>I’ve also used ETL to hapi but only did this for terminology resources.</p>\n<p>I’ve also streamed data in from hl7v2 messages. These are converted to transaction bundles and these are sent to hapi jpa.</p>",
        "id": 217488525,
        "sender_full_name": "Kevin Mayfield",
        "timestamp": 1605941229
    },
    {
        "content": "<p>Hey, thanks for your replies!<br>\nI also tried the facade approach at one time but the problem is that I can't write to the given database. That's why I'm trying to transfer it to a dedicated HAPI instance, so I can theoretically combine multiple sources at a later time. So I guess the answer is to write a custom service layer to convert the resources. You all work in Java, right? I was hoping that I can use NodeJS.</p>",
        "id": 217604742,
        "sender_full_name": "Florian Strieg",
        "timestamp": 1606123545
    }
]