[
    {
        "content": "<p>Anyone know how many HL7 messages a large hospital system might see in a day?</p>",
        "id": 153819377,
        "sender_full_name": "Mat Coolidge ",
        "timestamp": 1459435780
    },
    {
        "content": "<p>Yes, but there's a big \"depends.\" At a large public hospital in NYC we saw about 700,000 messages per day which accounted for almost all unique EHR transactions - including ADT activity of about 15,000 per weekday. A large portion of that activity involved the 100 or so clinics operating under the same EHR. Also, most EHRs, aside from McKesson's, won't dump out all transactions so you might get just diagnostic and medication traffic without most documentation events (clinical progress notes from the various caregivers). In any established environment there will be separate HL7 streams to and from each ancillary system: radiology, pharmacy, lab, cardiology, etc. If you count each message from those you'd end up with more messages per day because of the redundancy, perhaps.</p>",
        "id": 153819378,
        "sender_full_name": "pete welch",
        "timestamp": 1459436423
    },
    {
        "content": "<p>Thanks Pete! We were just spitballing some numbers to compare to our web service traffic</p>",
        "id": 153819411,
        "sender_full_name": "Mat Coolidge ",
        "timestamp": 1459438499
    },
    {
        "content": "<p>We are currently pushing around 200k rest requests per day</p>",
        "id": 153819412,
        "sender_full_name": "Mat Coolidge ",
        "timestamp": 1459438543
    },
    {
        "content": "<p>That sounds about right for our large hospital in Toronto too.</p>",
        "id": 153819434,
        "sender_full_name": "James Agnew",
        "timestamp": 1459447100
    },
    {
        "content": "<p>I've heard of &gt;20,000,000 per day</p>",
        "id": 153819445,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459453271
    },
    {
        "content": "<p>across a network </p>",
        "id": 153819446,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459453277
    },
    {
        "content": "<p>and this is the scale at which the NHS backbone operates</p>",
        "id": 153819447,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459453308
    },
    {
        "content": "<p>Grahame, in my example I helped build the specialized HL7 \"everything\" interface. Each clinical transaction in the EHR - order, result, documentation event  and so on, triggered an outbound message that I parsed into various data marts. Hospital size - about 750 beds. To get more than the 700,000 per day would mean counting redundant message streams - like pharmacy that could have a stream to a Pyxis system and an equivalent stream to a medication dispensing robot but not enough to get to your number. You might be referring to some of the larger hospital systems which would indeed have a huge combined traffic at the 20,000,000 per day level.</p>",
        "id": 153819472,
        "sender_full_name": "pete welch",
        "timestamp": 1459455987
    },
    {
        "content": "<p>well, combined healthcare systems - multiple hospitals across a region, or a single server for a whole state. </p>",
        "id": 153819473,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459456848
    },
    {
        "content": "<p>Yes, Mat did say large hospital system -- I interpreted that as a single hospital. A good ballpark would be 500,000 messages per hospital. </p>",
        "id": 153819481,
        "sender_full_name": "pete welch",
        "timestamp": 1459457995
    },
    {
        "content": "<p>This does lead to a follow-up question: is the REST/FHIR combo capable of that scale? Ewout's server does about 4 fhir message bundle posts per second on average hardware (8 cores, 16GB ram) in our current testing. </p>",
        "id": 153819483,
        "sender_full_name": "pete welch",
        "timestamp": 1459458344
    },
    {
        "content": "<p>technically, yes, it's capable of that. but a particular server solution has it's own set of choices to make. the general purpose servers will be slower, since they handle information generically, and over-index. </p>",
        "id": 153819484,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459458439
    },
    {
        "content": "<p>my server can do 10 transactions a second (enough for that scale) on my laptop, but it's prone to deadlock issues. </p>",
        "id": 153819485,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459458493
    },
    {
        "content": "<p>but you can throw resources at the problem, parallelise parts of it, pare back the indexing, optimise for the specific problem... </p>",
        "id": 153819487,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459458557
    },
    {
        "content": "<p>in principle, REST can scale to social web levels- it's about the what you do behind the front door </p>",
        "id": 153819488,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459458588
    },
    {
        "content": "<p>That's where the heavy load of fhir resource validation/conformance shows up, I guess.</p>",
        "id": 153819489,
        "sender_full_name": "pete welch",
        "timestamp": 1459458650
    },
    {
        "content": "<p>yes. validation too - how much of it do you want to do in production. often the answer turns out to be, none</p>",
        "id": 153819490,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459458802
    },
    {
        "content": "<p>These transaction \"messages\" very much depend on the resource content also.<br>\nHandling a 80k concept codesystem resource (single resource) might take some time to process in, vs a simple POST of an observation. And then there are all the search/get requests that shouldb be much faster as there are no (or little) updates invovled.</p>",
        "id": 153819499,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1459462085
    },
    {
        "content": "<p>As Grahame points out, its what the server does behind the wall.</p>",
        "id": 153819500,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1459462100
    },
    {
        "content": "<p>(UK NHS Hospital/trust) Message (hl7v2) transactions: about 100K messages (5-10 Gig Byte). Don't have transaction figures for non messaging systems (mainly FHIR) but it's handled 30+ Gig of data fine per day. </p>",
        "id": 153819599,
        "sender_full_name": "Kevin Mayfield",
        "timestamp": 1459488720
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-email=\"grahame@healthintersections.com.au\">@Grahame Grieve</span>: for what scenario would you propose no validation in production? </p>",
        "id": 153819637,
        "sender_full_name": "Paul Knapp",
        "timestamp": 1459515802
    },
    {
        "content": "<p>well, what I've often seen is that you have a double of validation - the full validation, and the business rules validation. If you work up the system using full valdiation in prototyping, it should all be good in production. But often it's not; it turns out that real daily clerical practice violates the assumptions that were made, and applying validation introduces operational issues, while not solving any problems. Often, in these caes, people start turning validation pieces off, and then eventually all of it </p>",
        "id": 153819670,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459572240
    },
    {
        "content": "<p>note that I'm not proposing anything. I'm merely observing</p>",
        "id": 153819671,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1459572249
    },
    {
        "content": "<p>You are referring to content models exchanged within an organization? If so I can see where that can make sense, in particular because 'what you are doing ' is actual not theoretical. But for content models exchanged between parties there will be some form of relatively complete validation and here again how that is implemented depends on many factors - xml validation requires a lot of additional machinery to try to get anywhere near comprehensive so if people choose not to implement that then what you often see is the least possible xml validation and all the rest at the business level. Either way for processing systems they may record that they got something before that validate but I expect they will validate completely, whatever that means for them, before they process or before they commit the processing.</p>",
        "id": 153819792,
        "sender_full_name": "Paul Knapp",
        "timestamp": 1459688687
    },
    {
        "content": "<p>well, perhaps a way to express is that often, people start out doing comprehensive validation and then they reduce it to the minumum needed in order to accomplish their task. note that I don't recomend this; I merely observee that it happens</p>",
        "id": 153820382,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1460062068
    }
]