[
    {
        "content": "<p>hi, I have originally posted this question to HAPI group.  This group appears to be a more correct location.  </p>\n<p>Hi,</p>\n<p>We are evaluating FHIR as a workflow management backend.  I have written a simple CSV import into FHIR system and I have run it against Spark server and I am running it against HAPI server 3.5.0.</p>\n<p>I observed that import performance is one of the slowest I have ever worked with.  So, I am looking at figuring out how FHIR import process can be done faster.</p>\n<p>Here is a high level algorithm I have applied so far:</p>\n<p>For each CSV record:</p>\n<p>For each FHIR resource type in CSV record:</p>\n<p>1. Build FHIR object from CSV data<br>\n2. Derive unique FHIR identifier from CSV data<br>\n3. Search by unique identifier in FHIR service to find if resource already exists.  If resource exists, make update call with FHIR object's id set to found id.  If object is not found, don't set id and create a new object in FHIR data store.</p>\n<p>Here is a log of the sequence used to create different resource types:</p>\n<ul>\n<li>Location: updated existing ID 4952</li>\n<li>Device: updated existing ID 5124</li>\n<li>Patient: created with ID 88366</li>\n<li>Encounter: created with ID 88367</li>\n<li>Practitioner: updated existing ID 40318</li>\n<li>Practitioner: updated existing ID 7</li>\n<li>Organization: updated existing ID 4957</li>\n<li>Organization: updated existing ID 4958</li>\n<li>Specimen: created with ID 88368</li>\n<li>ProcedureRequest: created with ID 88369</li>\n<li>Procedure: created with ID 88370</li>\n<li>ImagingStudy: created with ID 88371</li>\n<li>DiagnosticReport: created with ID 88372</li>\n<li>Task: created with ID 88373</li>\n<li>Task: created with ID 88374</li>\n<li>Task: created with ID 88375</li>\n<li>Task: created with ID 88376</li>\n<li>Task: created with ID 88377</li>\n<li>Task: created with ID 88378</li>\n<li>Task: created with ID 88379<br>\nFinished 5711 row; duration: 00:00:20.1776926</li>\n</ul>\n<p>From the log statement above you can see that:<br>\nI am on row #5711 and the import time is 20 seconds for 1 CSV record!  The import started at below 1 second on default HAPI installation.<br>\nI made 20 search calls, 6 update calls that generated new FHIR object version while data has not changed, 14 create calls.<br>\nObviously this kind of import process is inefficient and I am looking into ways to improve it.  </p>\n<p>As I understand Bundle could be the answer, but I am not quite sure how to specify id for object's that don't exist yet.</p>\n<p>So, any advice is appreciated.  </p>\n<p>Thank you.</p>",
        "id": 154010248,
        "sender_full_name": "Sergei Gnezdov",
        "timestamp": 1539889267
    },
    {
        "content": "<p>Sounds interesting - if we have Bulk Export we should have Bulk Import API :)</p>",
        "id": 154010257,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1539890561
    },
    {
        "content": "<p>Agreed -- this has come up in discussions on the export side, and I think there'd be a pretty natural analog that we can define.</p>",
        "id": 154010266,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1539893148
    },
    {
        "content": "<p>A couple of things that might bridge the gap between \"full blown bulk import service\" (which would be great) and your current level of performance:</p>\n<ul>\n<li>The default config for HAPI is definitely not tuned for write performance. It has tons of search parameters enabled (basically all of the built-in FHIR ones), fulltext indexing enabled, \"missing search param\" indexing enabled, etc. You should consider disabling any search parameters you don't need, disabling lucene if you're not using <code>_text</code>, etc.</li>\n<li>The workflow of doing a whole bunch of searches and creates/updates individually for each row sounds like a killer for performance. You should consider using FHIR transactions with features like placeholder IDs between your resources, conditional creates, conditional updates, etc. This lets HAPI optimize quite a bit in terms of how the write works.</li>\n</ul>",
        "id": 154010273,
        "sender_full_name": "James Agnew",
        "timestamp": 1539897976
    },
    {
        "content": "<p>I think, efficient bulk import can sometimes for performance go directly into database with some asynchronous (deferred) validation</p>",
        "id": 154010443,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1539960690
    },
    {
        "content": "<p>yeah, a mechanism for that would be much better still, Id agree..</p>",
        "id": 154010677,
        "sender_full_name": "James Agnew",
        "timestamp": 1539987718
    },
    {
        "content": "<p>That's what ours does, it processes the import file in several passes, then is a direct import into our native table format.</p>",
        "id": 154010723,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1540015525
    },
    {
        "content": "<p>We have an implementation of HAPI fhir based on a plain server (not jpa) and it's trivial<br>\nto bulk load in the database thought basic ETL. BTW the database backend is a hadoop <br>\n10 computer cluster with apache Phoenix  combined to a apache solR for _text filtering.<br>\nOnce all resource implemented, that's confortable to work with.</p>",
        "id": 154010729,
        "sender_full_name": "natus",
        "timestamp": 1540028355
    },
    {
        "content": "<p>Hi all, I think I can get good advice from this group.<br>\nI'm transforming EHR to FHIR. At first, I start with the small size of EHR, and I used SQL procedures and a couple of bash script. <br>\nBut I have to handle much larger volumes(terabytes).<br>\nHave you used big data backend like Apache Kafka, Apache NiFi to do ETL?<br>\nI found this document, but it would like to know other references from this group. <a href=\"https://community.hortonworks.com/articles/138249/nifi-in-healthcare-ingesting-hl7-data-in-nifi.html\" target=\"_blank\" title=\"https://community.hortonworks.com/articles/138249/nifi-in-healthcare-ingesting-hl7-data-in-nifi.html\">https://community.hortonworks.com/articles/138249/nifi-in-healthcare-ingesting-hl7-data-in-nifi.html</a></p>",
        "id": 154010735,
        "sender_full_name": "Cinyoung Hur",
        "timestamp": 1540035094
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"196774\">@Cinyoung Hur</span> From my side the ETL has focused on batch processing though apache hive/spark/sqoop to load the historical part (multiple terabytes of RDBMS EHR databases). Kafka, nifi, spark streaming are more relevant for ingesting data stream in realtime but I have not right now plugged them on.</p>",
        "id": 154010760,
        "sender_full_name": "natus",
        "timestamp": 1540071768
    },
    {
        "content": "<p>@natus How often do you batch processing?</p>",
        "id": 154010771,
        "sender_full_name": "Cinyoung Hur",
        "timestamp": 1540093287
    },
    {
        "content": "<p>Right now, on a daily basis</p>",
        "id": 154010777,
        "sender_full_name": "natus",
        "timestamp": 1540112746
    },
    {
        "content": "<p>Team,</p>\n<div class=\"codehilite\"><pre><span></span><code>We need to use FHIRloader to do multi loads. How can we integrate this with Azure Datafactory or Databricks to perform bulk upload to FHIR server\n</code></pre></div>",
        "id": 249571181,
        "sender_full_name": "sandeep diddi",
        "timestamp": 1629109360
    },
    {
        "content": "<p>What is FHIRloader?</p>",
        "id": 249589290,
        "sender_full_name": "Lloyd McKenzie",
        "timestamp": 1629121065
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191320\">@Lloyd McKenzie</span> Its a tool from Microsoft for Bulk load to FHIR</p>",
        "id": 249601221,
        "sender_full_name": "sandeep diddi",
        "timestamp": 1629126547
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"241501\">@Caitlin Voegele</span> <span class=\"user-mention\" data-user-id=\"195075\">@Brendan Kowitz</span></p>",
        "id": 249601863,
        "sender_full_name": "Gino Canessa",
        "timestamp": 1629126823
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"428612\">sandeep diddi</span> <a href=\"#narrow/stream/179166-implementers/topic/bulk.20data.20import/near/249571181\">said</a>:</p>\n<blockquote>\n<p>Team,</p>\n<div class=\"codehilite\"><pre><span></span><code>We need to use FHIRloader to do multi loads. How can we integrate this with Azure Datafactory or Databricks to perform bulk upload to FHIR server\n</code></pre></div>\n\n</blockquote>\n<p>If you are looking at this tool here: <a href=\"https://github.com/microsoft/fhir-loader\">https://github.com/microsoft/fhir-loader</a>, please feel free to open an issue against the GitHub repo with questions so that one of the implementers of the tool can review. There are also examples on the page about the architecture.</p>",
        "id": 249604941,
        "sender_full_name": "Caitlin Voegele",
        "timestamp": 1629128126
    }
]