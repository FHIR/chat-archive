[
    {
        "content": "<p>For bulk serialization (to support research use cases), has there been any consideration or talk of FHIR supporting parquet to maximize efficiency? </p>",
        "id": 153880844,
        "sender_full_name": "Michelle (Moseman) Miller",
        "timestamp": 1491255873
    },
    {
        "content": "<p>parquet?</p>",
        "id": 153880851,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491257662
    },
    {
        "content": "<p>Presumably <a href=\"https://parquet.apache.org/\" target=\"_blank\" title=\"https://parquet.apache.org/\">https://parquet.apache.org/</a></p>",
        "id": 153880853,
        "sender_full_name": "Joel Schneider",
        "timestamp": 1491257866
    },
    {
        "content": "<p>given that FHIR is based on tree structures with internal cardinality &gt; 1, tables are problematic</p>",
        "id": 153880854,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491257940
    },
    {
        "content": "<p>A parquet schema can specify \"repeated\" fields, which could be a straightforward way to handle cardinality &gt; 1.</p>",
        "id": 153880858,
        "sender_full_name": "Joel Schneider",
        "timestamp": 1491259102
    },
    {
        "content": "<p>nested repeated? </p>",
        "id": 153880859,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491259193
    },
    {
        "content": "<p>I'm not an expert, but the doc says, \"Parquet is built from the ground up with complex nested data structures in mind ...\"  <a href=\"https://parquet.apache.org/documentation/latest/\" target=\"_blank\" title=\"https://parquet.apache.org/documentation/latest/\">https://parquet.apache.org/documentation/latest/</a></p>",
        "id": 153880860,
        "sender_full_name": "Joel Schneider",
        "timestamp": 1491259294
    },
    {
        "content": "<p>hmm. well, it would be interesting to investigate. As is also the R data_gram format</p>",
        "id": 153880861,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491259529
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> How can we help with the investigation process?  We have an internal proof of concept of sorts where we are using Parquet files with (Apache) Spark.  It is based on FHIR resource schema.  Would it help if I emailed you a couple example Parquet files?</p>",
        "id": 153880873,
        "sender_full_name": "Michelle (Moseman) Miller",
        "timestamp": 1491268489
    },
    {
        "content": "<p>it would help if they were publicly available. let me look at them and then propose a way to engage the community</p>",
        "id": 153880881,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491275253
    },
    {
        "content": "<p>Interested to hear what particular research use cases are being tracked; relating to drug trial usage or more population health research?</p>",
        "id": 153880925,
        "sender_full_name": "Geoff Low",
        "timestamp": 1491325294
    },
    {
        "content": "<p>Hi, I'm a colleague of Michelle's and have some context behind this question. We've found Parquet to be interesting because: </p>\n<p>1.) It is flexible enough to encode FHIR resources with arbitrary nested or repeated elements. I've attached examples of a (tiny) Parquet file of Observation resources and the Parquet schema it uses. (The schema itself was generated from resource definitions, and the Parquet file was generated by code that encodes FHIR resources using the HAPI FHIR API. This is proof-of-concept work so I can't rule out bugs in this yet, but we will be able to share/open source/contribute this if that would be of interest.)</p>\n<p>2.) Parquet is an efficient binary format with significant mind share in many analytic tools, and can be easily and efficiently loaded into them and queried. </p>\n<p>Our specific use case is centered around deep analytics of population health data. The ability to share very large FHIR datasets that can be imported and queried supports that, and surely a number of other needs as well. </p>\n<p>Side node: Since Parquet is binary format, users will need something like parquet-tools (linked below) to inspect it directly, or import it into a system that understands the format.</p>\n<p>In any case, I'm happy to engage on this topic in anyway that could help. </p>\n<p>[1]<br>\n<a href=\"https://github.com/apache/parquet-mr/tree/master/parquet-tools\" target=\"_blank\" title=\"https://github.com/apache/parquet-mr/tree/master/parquet-tools\">https://github.com/apache/parquet-mr/tree/master/parquet-tools</a></p>\n<p>EDIT: might be having trouble with the attachment, but here is a link to the Parquet schema generated from the resource definition: <a href=\"https://gist.github.com/rbrush/af4b8f7bed8ec3cd5fc34419325c4e86\" target=\"_blank\" title=\"https://gist.github.com/rbrush/af4b8f7bed8ec3cd5fc34419325c4e86\">https://gist.github.com/rbrush/af4b8f7bed8ec3cd5fc34419325c4e86</a></p>\n<p><a href=\"/user_uploads/10155/oajiDFsSCJ8GOuxxsSEA8VrS/obs_example_parquet_schema.txt\" target=\"_blank\" title=\"obs_example_parquet_schema.txt\">obs_example_parquet_schema.txt</a> </p>\n<p><a href=\"/user_uploads/10155/bgCCc1eB8zJf8jwf2fW06sH-/obs_example.snappy.parquet\" target=\"_blank\" title=\"obs_example.snappy.parquet\">obs_example.snappy.parquet</a></p>",
        "id": 153880930,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1491337575
    },
    {
        "content": "<p>thanks, can you comment on how the parquet format can be used? how many analytical tools support it? How about R and the related AI community</p>",
        "id": 153881051,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491438398
    },
    {
        "content": "<p>Parquet is commonly used in conjunction with Spark (<a href=\"http://spark.apache.org\" target=\"_blank\" title=\"http://spark.apache.org\">spark.apache.org</a>) and/or Hadoop.  That ecosystem also includes Spark/R (<a href=\"http://spark.apache.org/docs/latest/sparkr.html\" target=\"_blank\" title=\"http://spark.apache.org/docs/latest/sparkr.html\">http://spark.apache.org/docs/latest/sparkr.html</a>).</p>",
        "id": 153881164,
        "sender_full_name": "Joel Schneider",
        "timestamp": 1491511440
    },
    {
        "content": "<p>I can't find an assigned mime for Parquet format?</p>",
        "id": 153881182,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491515590
    },
    {
        "content": "<p>I couldn't find an assigned MIME type for Parquet format either, although the related Apache Thrift format does have assigned mimes.  For FHIR, maybe application/fhir+parquet would make sense.</p>",
        "id": 153881195,
        "sender_full_name": "Joel Schneider",
        "timestamp": 1491519702
    },
    {
        "content": "<p>As for how Parquet is used: it can be read in R and converted to data frames via Apache Spark, but is also natively supported by a number of open source tools (Apache projects like Spark, Impala, and Presto, as well as Java, Python and C++ libraries) and several scalable database systems (like Amazon Athena, Google BigQuery, Microsoft PolyBase, and HP Vertica). I'm sure I'm missing others.</p>\n<p>Our goal is to share large datasets (many terabytes) with users looking to do deep analysis in a standard format that is easily plugged into a variety of tools, and FHIR-in-Parquet makes for a good combination of standards, scalability, and mindshare across systems.</p>",
        "id": 153881207,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1491528320
    },
    {
        "content": "<p>it's certainly a problem that we're interested in </p>",
        "id": 153881209,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1491531555
    }
]