[
    {
        "content": "<p>Pre-print article from Friday: <a href=\"https://arxiv.org/pdf/1801.07860.pdf\" target=\"_blank\" title=\"https://arxiv.org/pdf/1801.07860.pdf\">Scalable and accurate deep learning for electronic health records</a> shows (among many other things) where Google's using FHIR in deepn learning for EHRs.</p>\n<p>It's super exciting to see this work shared! Congrats <span class=\"user-mention\" data-user-id=\"194832\">@Eyal Oren</span> and  <span class=\"user-mention\" data-user-id=\"194833\">@Patrik Sundberg</span>!</p>",
        "id": 153932827,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1517237338
    },
    {
        "content": "<p>This is amazing, is Tensorflow on FHIR next? I wish there were/hope there will be more details about the data pre-processing into FHIR. Particularly how they made or represented the clinical notes.</p>",
        "id": 153932938,
        "sender_full_name": "Travis Stenerson",
        "timestamp": 1517245097
    },
    {
        "content": "<p>Hey Travis, yes, we're using TF on top of FHIR but opening that up publicly is a bit tricky (standard models don't work very well for this kind of data, for various reasons I can go into). We're planning to though. As for details on data-processing, there is a (somewhat short) description in the \"methods\" section, but we may publish a more extensive description. Re:  notes .. we just read them left to right ;-) The sequence of tokens within each notes is maintained, as is the sequence of notes, and the relative timestamp of notes (as are all the relative timestamps of all data). Then we \"just\" train, with various architectures as described, including eg LSTMs.</p>",
        "id": 153933107,
        "sender_full_name": "Eyal Oren",
        "timestamp": 1517263142
    },
    {
        "content": "<p>Not often you get an immediate reply from one of lead authors! Incredible work, congrats! Those tokens I guess were just key words encountered repeatedly? Did you use a terminology service to link synonyms? I read the short section in appendix A, but definitely will be reading the rest, especially about the architecture of the feed forward model used, as I tried and failed to use one on time series diabetes data. I have a bit of an idea of why this kind of data doesn't work well with standard models, but I'd still love to hear the reasons.</p>",
        "id": 153934943,
        "sender_full_name": "Travis Stenerson",
        "timestamp": 1517553570
    },
    {
        "content": "<p>\"we did not harmonize elements to a standard terminology or ontology\"<br>\nThis makes me smile, there is so much room to grow from here</p>",
        "id": 153934997,
        "sender_full_name": "Alex Sullivan",
        "timestamp": 1517584872
    },
    {
        "content": "<blockquote>\n<p>Not often you get an immediate reply from one of lead authors! Incredible work, congrats! Those tokens I guess were just key words encountered repeatedly? Did you use a terminology service to link synonyms? I read the short section in appendix A, but definitely will be reading the rest, especially about the architecture of the feed forward model used, as I tried and failed to use one on time series diabetes data. I have a bit of an idea of why this kind of data doesn't work well with standard models, but I'd still love to hear the reasons.</p>\n</blockquote>\n<p>sorry for late reply, didn't see this previously. For noten, each word becomes a token, in sequence. Not terminology services are used, the model learns synonyms (google: embeddings and word2vec for more info)</p>",
        "id": 153941838,
        "sender_full_name": "Eyal Oren",
        "timestamp": 1520023275
    },
    {
        "content": "<blockquote>\n<p>\"we did not harmonize elements to a standard terminology or ontology\"<br>\nThis makes me smile, there is so much room to grow from here</p>\n</blockquote>\n<p>Thanks!</p>",
        "id": 153941839,
        "sender_full_name": "Eyal Oren",
        "timestamp": 1520023315
    }
]