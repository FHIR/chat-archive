[
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191356\">@Jenni Syed</span> , do you consider to enable gzip http compression on Cerner staging Bulk API server? It will drastically decrease amounts of data being downloaded, from hundreds of megabytes to just tens.</p>",
        "id": 153995781,
        "sender_full_name": "Mikhail Lapshin",
        "timestamp": 1536682773
    },
    {
        "content": "<p>The current testing implementation is returning static links to direct downloads from S3.  We could put those links behind CloudFront and enable gzip compression.  I think it's also possible to just upload gzipped data but that'd limit access to uncompressed data for anybody who'd want it</p>",
        "id": 153995869,
        "sender_full_name": "Dennis Patterson",
        "timestamp": 1536694137
    },
    {
        "content": "<p>I think most of HTTP clients (including browsers) understand gzip :)</p>",
        "id": 153995871,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1536694234
    },
    {
        "content": "<p>part of this limitation is just because this is our beta implementation - not production ready. The gzip and several other http level \"large file handling\" considerations would be beneficial to add to considerations in the spec</p>",
        "id": 153995873,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1536694441
    },
    {
        "content": "<p>eg: streaming :)</p>",
        "id": 153995874,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1536694447
    },
    {
        "content": "<p>this sounds like a good topic for the connectathon :)</p>",
        "id": 153995875,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1536694466
    },
    {
        "content": "<p>I know we've talked about chunking as well in the past, which is something we support on some of our other APIs for large file transfers (each chunk would then be gzip as well)</p>",
        "id": 153995878,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1536694603
    },
    {
        "content": "<p>Also, I know you mentioned megabytes because that's likely the amount of data we have here. What we've seen in other settings/more realistic production settings gets into the gig size ranges</p>",
        "id": 153995882,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1536695096
    },
    {
        "content": "<p>(with other bulk APIs we have that do similar types of operations)</p>",
        "id": 153995883,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1536695135
    },
    {
        "content": "<blockquote>\n<p>The current testing implementation is returning static links to direct downloads from S3.  We could put those links behind CloudFront and enable gzip compression.  I think it's also possible to just upload gzipped data but that'd limit access to uncompressed data for anybody who'd want it</p>\n</blockquote>\n<p>S3 can properly serve pre-gzipped files, you just need to set 'Content Type' and 'Content Encoding' properties on S3 files as described in this post: <a href=\"https://medium.com/@graysonhicks/how-to-serve-gzipped-js-and-css-from-aws-s3-211b1e86d1cd\" target=\"_blank\" title=\"https://medium.com/@graysonhicks/how-to-serve-gzipped-js-and-css-from-aws-s3-211b1e86d1cd\">https://medium.com/@graysonhicks/how-to-serve-gzipped-js-and-css-from-aws-s3-211b1e86d1cd</a></p>\n<p>It took me almost a day to download all 5GBs of this dataset, that's why I'm so concerned :)</p>",
        "id": 153996193,
        "sender_full_name": "Mikhail Lapshin",
        "timestamp": 1536748543
    },
    {
        "content": "<p>is there use of http/2 which includes multi-threading and automatic compression?</p>",
        "id": 153996268,
        "sender_full_name": "John Moehrke",
        "timestamp": 1536763863
    },
    {
        "content": "<p>We haven't talked about needing that yet in the spec (much like GZip, streaming, etc) and good discussion to have. I will say that HTTP 2 support is still a bit spotty. I think under 30% support it as of the last stat I heard? So it may not be the 100% win here.</p>",
        "id": 153996285,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1536766357
    },
    {
        "content": "<p>understood, just wanting it on the stack of things to consider... especially when the group is considering gzip.</p>",
        "id": 153996291,
        "sender_full_name": "John Moehrke",
        "timestamp": 1536767052
    },
    {
        "content": "<p>Set up a CloudFront instance in front of our S3 bucket to auto-compress.  Turns out they'll only do this for files smaller than 10MB, so not gonna help :).  We'll have to look at uploading pre-compressed contents</p>",
        "id": 153996643,
        "sender_full_name": "Dennis Patterson",
        "timestamp": 1536847357
    },
    {
        "content": "<p>Interesting -- and that would be an API change (i.e., it changes what's returned). I was assuming <code>Accept-Encoding: gzip</code> would get us where we needed to be, but evidently not with S3 http hosting?</p>",
        "id": 153996669,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1536851152
    },
    {
        "content": "<p>Noting that this is all with pre-generated, mock data...  Per <span class=\"user-mention\" data-user-id=\"191332\">@Mikhail Lapshin</span> 's comments above, I think it'd be uploading the gzipped data, tell S3 to return Content-Encoding: gzip, and then when we return the list of files, they'd be able to be retrieved when requesting Accept-Encoding: gzip.  I think AWS' approach is more elaborate if you want to return various compressions (i.e. store them all pre-compressed in S3 and use Lambda@Edge to vary what gets returned according to Accept-Encoding...blah)</p>",
        "id": 153996676,
        "sender_full_name": "Dennis Patterson",
        "timestamp": 1536851424
    },
    {
        "content": "<p>In this scenario I think things would fail in the <em>absence</em> of <code>Accept-Encoding: gzip</code>-- because a client would get gzipped content regardless of what they requested.</p>",
        "id": 153996690,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1536852149
    },
    {
        "content": "<p>Right, unless we did the work to support both, that's correct.  This would be a connectathon-only limitation for our server, but from the very presence of this thread, I'm guessing that's what most clients want :)</p>",
        "id": 153996693,
        "sender_full_name": "Dennis Patterson",
        "timestamp": 1536852412
    },
    {
        "content": "<p>It's definitely what most clients <em>should</em> want :-)</p>",
        "id": 153996694,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1536852430
    }
]