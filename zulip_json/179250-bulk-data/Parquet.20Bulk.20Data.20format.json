[
    {
        "content": "<p>Hi All. It was good meeting many of you at the EU DevDays last week, and thanks for attending my talk on our FHIR/Parquet <br>\nproject.</p>",
        "id": 181828478,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1574693229
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191315\">@Josh Mandel</span> - per our discussion, off-the-shelf Apache Spark can transform NDJSON to Parquet files.<br>\nI've checked that these commands work (with Synthea patient data):</p>\n<div class=\"codehilite\"><pre><span></span>scala&gt;  val patients= spark.read.json(&quot;/path/patients.json&quot;)\nscala&gt; patients.write.parquet(&quot;/path/patients.parquet&quot;)\n</pre></div>\n\n\n<p>The Parquet data set schema is identical to the NDJSON source:</p>\n<div class=\"codehilite\"><pre><span></span>scala&gt; val p_patients = spark.read.parquet(&quot;/path/patients.parquet&quot;)\nscala&gt; val s1 = patients.schema.fields.map(f =&gt; (f.name, f.nullable))\nscala&gt; val s2 = p_patients.schema.fields.map(f =&gt; (f.name, f.nullable))\nscala&gt; s1.diff(s2).isEmpty\nres1: Boolean = true\n</pre></div>\n\n\n<p>Also, the data is the same:</p>\n<div class=\"codehilite\"><pre><span></span>scala&gt; p_patients.except(patients).count\nres2: Long = 0\n</pre></div>\n\n\n<p>So it looks like the data in Parquet files is a perfect copy of the data in NDJSON original.</p>",
        "id": 181828515,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1574693258
    },
    {
        "content": "<p>The next steps could be:</p>\n<p>Phase 1) We'll investigate the following performance aspects of Parquet conversion:<br>\na) compression (how much transmission bandwidth is saved in SNAPPY and GZip compression modes, defined in Parquet standard)<br>\nb) SQL query efficiency on exported data (columnar projection and predicate pushdown can be imperfect when executed on nested columns, we need to test this). <br>\nOnce we have enough data and are comfortable with the results, we might add the highlights <br>\nto the FHIR spec draft. Since NDJSON and Parquet schema/data are identical, maybe Parquet could then be moved from candidates to the list of bulk formats (spec draft), for further validation by the community. <br>\nNo other changes are required, other than allowing 'parquet' value in the accept/_outputFormat parameters (plus, optionally, adding the compression mode in the request).</p>\n<p>Phase 2) Per your suggestion, we will then add an advanced feature where the receiver of bulk export could specify the <br>\nrequired Parquet schema.<br>\nIf not specified, the Parquet schema will be identical to the original.<br>\nHowever, a changed schema can help with analytics performance, since using the original schema might slow down the queries (item 1b above).<br>\nI guess we'll need the receiver to send a mapping of the original NDJSON schema to the required Parquet schema <br>\n(what to flatten, what to drop - and maybe which extensions should be made \"first class citizens\", per the sql-on-fhir work). Using FHIRPath , profiles, etc - requires investigation. Can also start with receiver requesting the original NDJSON schema of the data to be exported (to decide what schema to request / what to map).</p>\n<p>Phase 3) Once the Parquet encryption mechanism is released, we can add another advanced feature where the receiver of bulk export can require encryption of the exported data.<br>\nInitially, all columns can be encrypted - with different data keys, but with the same master key. The receiver will send <br>\nits public key to the FHIR server; the server will use it to encrypt the master key and send it with the data.<br>\nLater, we can enable the receiver to specify what columns to encrypt (the rest won't be encrypted). And/or - the FHIR <br>\nServer can be pre-configured with sensitivity of columns in each table, so the FHIR Server itself knows which columns must be encrypted.</p>\n<p>Does it make sense? Any feedback will be appreciated!</p>",
        "id": 181828611,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1574693302
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"197632\">@Gidon Gershinsky</span> What is the schema that you end up getting? How many columns does it generate?</p>",
        "id": 181855914,
        "sender_full_name": "Veliyan Georgiev",
        "timestamp": 1574710934
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"197402\">@Veliyan Georgiev</span> This is returned for both NDJSON and Parquet data:</p>\n<div class=\"codehilite\"><pre><span></span>scala&gt; patients.printSchema()\nroot\n |-- address: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- city: string (nullable = true)\n |    |    |-- country: string (nullable = true)\n |    |    |-- extension: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- extension: array (nullable = true)\n |    |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |    |    |-- valueDecimal: double (nullable = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |-- line: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- postalCode: string (nullable = true)\n |    |    |-- state: string (nullable = true)\n |-- birthDate: string (nullable = true)\n |-- communication: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- language: struct (nullable = true)\n |    |    |    |-- coding: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- code: string (nullable = true)\n |    |    |    |    |    |-- display: string (nullable = true)\n |    |    |    |    |    |-- system: string (nullable = true)\n |    |    |    |-- text: string (nullable = true)\n |-- extension: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- extension: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- url: string (nullable = true)\n |    |    |    |    |-- valueCoding: struct (nullable = true)\n |    |    |    |    |    |-- code: string (nullable = true)\n |    |    |    |    |    |-- display: string (nullable = true)\n |    |    |    |    |    |-- system: string (nullable = true)\n |    |    |    |    |-- valueString: string (nullable = true)\n |    |    |-- url: string (nullable = true)\n |    |    |-- valueAddress: struct (nullable = true)\n |    |    |    |-- city: string (nullable = true)\n |    |    |    |-- country: string (nullable = true)\n |    |    |    |-- state: string (nullable = true)\n |    |    |-- valueCode: string (nullable = true)\n |    |    |-- valueDecimal: double (nullable = true)\n |    |    |-- valueString: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- id: string (nullable = true)\n |-- identifier: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- system: string (nullable = true)\n |    |    |-- type: struct (nullable = true)\n |    |    |    |-- coding: array (nullable = true)\n |    |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |    |-- code: string (nullable = true)\n |    |    |    |    |    |-- display: string (nullable = true)\n |    |    |    |    |    |-- system: string (nullable = true)\n |    |    |    |-- text: string (nullable = true)\n |    |    |-- value: string (nullable = true)\n |-- maritalStatus: struct (nullable = true)\n |    |-- coding: array (nullable = true)\n |    |    |-- element: struct (containsNull = true)\n |    |    |    |-- code: string (nullable = true)\n |    |    |    |-- display: string (nullable = true)\n |    |    |    |-- system: string (nullable = true)\n |    |-- text: string (nullable = true)\n |-- multipleBirthBoolean: boolean (nullable = true)\n |-- name: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- family: string (nullable = true)\n |    |    |-- given: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- prefix: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- use: string (nullable = true)\n |-- resourceType: string (nullable = true)\n |-- telecom: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- system: string (nullable = true)\n |    |    |-- use: string (nullable = true)\n |    |    |-- value: string (nullable = true)\n |-- text: struct (nullable = true)\n |    |-- div: string (nullable = true)\n |    |-- status: string (nullable = true)\n</pre></div>",
        "id": 181861753,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1574715308
    },
    {
        "content": "<p>Thanks for the follow-up here <span class=\"user-mention\" data-user-id=\"197632\">@Gidon Gershinsky</span>, and great meeting you too. Re: schema, I'm still not 100% sure what's happening under the hood in this example. For example, is the <code>.write.parquet()</code>inspecting the data to come up wtih a schema? Will the schema be different for different NDJSON files (e.g., one full of <code>Patient</code> resoures vs one full of <code>Observation</code> resources? Or one full of <code>Observation</code>s with <code>.component</code> elements vs one full of <code>Observation</code>s without <code>.component</code> elemenents? Or one with 2-level-deep <code>Questionnaire</code> items vs 3-level-deep?)</p>",
        "id": 181866888,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1574718530
    },
    {
        "content": "<p>In other words, how do the particular instance data shape the schema, and does it matter? Do consumers of the data need to care? Can data be combined arbitrarily without worrying about this, as long as the schema are \"compatible\" (like, they might differ but don't contradict each other by using the same field name to capture distinct data)?</p>",
        "id": 181866953,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1574718591
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191315\">@Josh Mandel</span> , the under the hood machinery is based on DataFrames, the main construct in Spark (SQL) - a distributed table <br>\nwith schema and data.<br>\nBoth <code>patient</code> and <code>p_patient</code> are DataFrame objects. When a DataFrame is created from an NDJSON file, as in the</p>\n<div class=\"codehilite\"><pre><span></span>val patients= spark.read.json(&quot;/path/patients.json&quot;)\n</pre></div>\n\n\n<p>the schema is inferred from the JSON dataset<br>\n<a href=\"https://spark.apache.org/docs/latest/sql-data-sources-json.html\" target=\"_blank\" title=\"https://spark.apache.org/docs/latest/sql-data-sources-json.html\">https://spark.apache.org/docs/latest/sql-data-sources-json.html</a></p>\n<p>A created DataFrame can then be saved in a file, eg in Parquet format:</p>\n<div class=\"codehilite\"><pre><span></span>patients.write.parquet(&quot;/path/patients.parquet&quot;)\n</pre></div>\n\n\n<p>This operation performs \"<em>writing Parquet files that automatically preserves the schema of the original data</em>\"<br>\n<a href=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\" target=\"_blank\" title=\"https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\">https://spark.apache.org/docs/latest/sql-data-sources-parquet.html</a></p>\n<p>Then, when we create a new DataFrame (in another process - the reader of Bulk Export data, eg Spark run by <br>\nresearchers etc) from this parquet file,</p>\n<div class=\"codehilite\"><pre><span></span>val p_patients = spark.read.parquet(&quot;/path/patients.parquet&quot;)\n</pre></div>\n\n\n<p>we get the exact copy of the original NDJSON data and schema. Btw, DataFrames loaded from Parquet files, get the<br>\nexplicit schema stored in Parquet metadata, instead of inferring it from data as done for NDJSON files.<br>\nIn any case, the consumers don't need to care, they automatically get the original schema, exactly<br>\nthe same as they have today with NDJSON export.</p>\n<p>So the schema of Patients and Observations will certainly be different.</p>\n<p>Regarding your questions on Observation components and Questionnaire level depth - I believe Spark will be <br>\nable to accurately infer the schema - to reflect the level depth or presence of components. However, <br>\nI want to verify that, will set up and run additional tests. </p>\n<p>There is also a question of different lines with a different schema in the same Resource. Eg all Observations <br>\nare exported as one NDJSON file; but some Observation lines have a <code>component</code> element, while others don't.<br>\n(if this is not allowed in FHIR - please let me know). <br>\nWhen loading this file as a DataFrame, Spark will likely create a wider schema - with the component column(s). <br>\nThey will be empty for rows without components. Again, something we need to verify, I'll check it .</p>\n<p>This becomes even more interesting when the full FHIR Server dataset (with all Resource types) is exported as one<br>\nNDJSON file (or a collection of NDJSON files, for staged download - but with each file having many Resource types).<br>\nThen the Spark will probably create a super-set of all schemas. We'll  verify the size and <br>\nefficiency implications when writing this data to Parquet. However, a simple fallback strategy<br>\nwould be to formally suggest/require Parquet bulk export implementations to create separate files for separate Resource <br>\ntypes. But again, TBD.</p>",
        "id": 181926166,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1574778702
    },
    {
        "content": "<p>Thanks -- this is really helpful background, and it'll be great to have the results from your verification tests (e.g., mixed resource types, etc). Note that in FHIR even for files with a single resource type (like, Observations.ndjson), each <code>Observation.contained[]</code> can contain an arbitrary list of FHIR \"contained\" resources of any time, so it'll be helpful to know the impacts of this on performance, too.</p>",
        "id": 181926445,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1574778901
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"197632\">@Gidon Gershinsky</span> Hi Gidon..this is very helpful.  Caution ahead - what I suspect might happen is that should you decide to do some unions or joins you might end up with errors. In example - the schema has multipleBirthBoolean but it doesn't have multipleBirthInteger. What happens under the hood is - as you read schema 1 the columns will be in a certain order based on that schema and when you read schema 2 - they will be different. When you do certain operations like Union - the columns will not match. There is an option to match by name rather than column index but it is flaky with FHIR (or at least it was for us). The best thing you can do is create your schema from a full (fake) FHIR bundle with all resources and all fields populated with something (especially the polymorphic properties like that one in example). This way the columns will always be in the proper order.</p>",
        "id": 182343782,
        "sender_full_name": "Veliyan Georgiev",
        "timestamp": 1575292848
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"197402\">@Veliyan Georgiev</span> Hi Veliyan, both NDJSON and Parquet work with column names, explicitly mapped to the<br>\n in-file data, so there wouldn't be a confusion related to column order etc. In your example, each Parquet file with <br>\nPatient data,  will have a column named multipleBirthBoolean (assuming it was present in the NDJSON original). In each <br>\nof these files, Parquet will be able to correctly set the value of this column for every row, independently of presence <br>\nor absence of multipleBirthInteger column/values in that file. This will allow unions and joins <br>\nto function properly. But I do agree that we need to proceed with caution. Per the discussion with Josh above, we are currently collecting a set of validation tests; all suggestions on what needs to be verified are very much welcome (preferably with details on how to run them, and what's the expected result).</p>",
        "id": 182455326,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1575384568
    },
    {
        "content": "<blockquote>\n<p>Phase 2) Per your suggestion, we will then add an advanced feature where the receiver of bulk export could specify the<br>\nrequired Parquet schema.<br>\n...<br>\nI guess we'll need the receiver to send a mapping of the original NDJSON schema to the required Parquet schema<br>\n(what to flatten, what to drop - and maybe which extensions should be made \"first class citizens\", per the sql-on-fhir work). Using FHIRPath , profiles, etc - requires investigation. Can also start with receiver requesting the original NDJSON schema of the data to be exported (to decide what schema to request / what to map).</p>\n</blockquote>\n<p>wouldn't it be useful to have some analytic-friendly schema pre-defined in spec?  i'm thinking something where spark SQL can operate over the format in a manner as similar as possible to (or preferably the same as) whats defined at <a href=\"https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md\" target=\"_blank\" title=\"https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md\">https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md</a><br>\nthen the client can just say \"i want the export in the analytic parquet format please\"?</p>",
        "id": 182774794,
        "sender_full_name": "Lee Surprenant",
        "timestamp": 1575648113
    },
    {
        "content": "<blockquote>\n<p>This becomes even more interesting when the full FHIR Server dataset (with all Resource types) is exported as one<br>\nNDJSON file (or a collection of NDJSON files, for staged download - but with each file having many Resource types).<br>\nThen the Spark will probably create a super-set of all schemas. We'll verify the size and<br>\nefficiency implications when writing this data to Parquet. However, a simple fallback strategy<br>\nwould be to formally suggest/require Parquet bulk export implementations to create separate files for separate Resource<br>\ntypes. But again, TBD.</p>\n</blockquote>\n<p>Per my understanding, FHIR Bulk export already creates only one resource type per file (even for ndjson), so I don't think Parquet would be any different</p>",
        "id": 182777320,
        "sender_full_name": "Lee Surprenant",
        "timestamp": 1575649620
    },
    {
        "content": "<p>see <a href=\"http://hl7.org/fhir/uv/bulkdata/STU1/export/index.html#response---complete-status\" target=\"_blank\" title=\"http://hl7.org/fhir/uv/bulkdata/STU1/export/index.html#response---complete-status\">http://hl7.org/fhir/uv/bulkdata/STU1/export/index.html#response---complete-status</a></p>\n<blockquote>\n<p>Each file SHALL contain resources of only one type, but a server MAY create more than one file for each resource type returned.</p>\n</blockquote>",
        "id": 182777646,
        "sender_full_name": "Lee Surprenant",
        "timestamp": 1575649809
    },
    {
        "content": "<blockquote>\n<p>see <a href=\"http://hl7.org/fhir/uv/bulkdata/STU1/export/index.html#response---complete-status\" target=\"_blank\" title=\"http://hl7.org/fhir/uv/bulkdata/STU1/export/index.html#response---complete-status\">http://hl7.org/fhir/uv/bulkdata/STU1/export/index.html#response---complete-status</a></p>\n<blockquote>\n<p>Each file SHALL contain resources of only one type, but a server MAY create more than one file for each resource type returned.</p>\n</blockquote>\n</blockquote>\n<p>Hi <span class=\"user-mention\" data-user-id=\"191676\">@Lee Surprenant</span> , thanks. I had a feeling it could be the case, but its good to know this is explicitly spelled out in the spec. In addition, we will  check the impact of <code>contained</code> resources on efficiency of Parquet storage, as suggested by Josh.</p>",
        "id": 182894563,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1575814496
    },
    {
        "content": "<blockquote>\n<p>wouldn't it be useful to have some analytic-friendly schema pre-defined in spec?  i'm thinking something where spark SQL can operate over the format in a manner as similar as possible to (or preferably the same as) whats defined at <a href=\"https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md\" target=\"_blank\" title=\"https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md\">https://github.com/FHIR/sql-on-fhir/blob/master/sql-on-fhir.md</a><br>\nthen the client can just say \"i want the export in the analytic parquet format please\"?</p>\n</blockquote>\n<p>We explore 2 applications of Parquet - one basic, for general Bulk Export (all usecases - not limited to analytics), and the other advanced, for Bulk Export optimized for analytics. Your comment refers to the second application, but let me step back and sum up both parts being discussed.</p>\n<p>\"General export\":  a user can request either \"ndjson\" or \"parquet\" format within the current API .  This doesn't require any change in the current spec, except for allowing a \"parquet\" value in the <code>accept</code> header and <code>_outputFormat</code> parameter. Plus potentially an addition of a <code>_compressionType</code> parameter. Why would a user (and a FHIR service provider) want a general-purpose parquet export? For the following reasons:</p>\n<ul>\n<li>\n<p>(much) less bytes are prepped/stored by FHIR server, and sent on wire to user - due to parquet encoding and compression</p>\n</li>\n<li>\n<p>faster security - instead of TLS handshakes on each file, TLS is performed once only (to get Parquet keys) - then the encrypted files are sent on a regular connection</p>\n</li>\n<li>\n<p>received files support columnar projection and predicate push down</p>\n</li>\n</ul>\n<p>The last bullet is a side benefit - while the \"general export\" parquet is better than ndjson for sql workloads, it is not as good as an advanced \"analytics-friendly export\" parquet which is optimized for these workloads. On the other hand, the \"general export\" parquet is lossless (data and schema identical to ndjson export; recursive schemas TBD), while the \"analytics\" parquet is lossy (per the  sql-on-fhir.md, some elements are dropped). <br>\nThe main bullets are the first two (faster transfer / less bandwidth &amp; storage). We will run some experiments to estimate the efficiency of the parquet format for a general lossless export. If efficient enough, it can be a \"low hanging fruit\" for the spec addition to start validation by the community.</p>\n<p>\"Analytics-friendly export\" : by default, a user requesting bulk export in parquet format, would get the lossless general-purpose files. To get the analytics-friendly files, the user would have to request this explicitly, maybe with additional parameters. Unlike the \"general export\" mode, this one requires substantial additions in the spec - to be chosen among many options that need to be discussed. I agree it might be possible to pre-define  (for each resource) an analytics-friendly schema in the spec, according to the sql-on-fhir.md recommendations, that already show how certain elements can be dropped and how extensions can be made first-class fields. It could be  a good starting point. But this is not the most efficient format for analytic frameworks like Apache Spark - that are not at their best with nested columns (still kept in sql-on-fhir.md), and run faster with a flat schema. Also, a user might be interested to drop many more elements upon export (if they are irrelevant for his analytic workloads). So, ideally, a user should be able to tell a FHIR server what parquet schema he is interested in (and how the original resources should be mapped to it). Or to choose from a number of pre-defined schemas / profiles.</p>",
        "id": 182897946,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1575820545
    },
    {
        "content": "<blockquote>\n<p>except for allowing a \"parquet\" value in the accept header and _outputFormat parameter. </p>\n</blockquote>\n<p>I think <em>just</em> the <code>_outputFormat</code> would change, not the <code>Accept</code> header, since the manifest of results would still be a JSON file with <code>output</code> entries.</p>",
        "id": 183008181,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1575931468
    },
    {
        "content": "<blockquote>\n<blockquote>\n<p>except for allowing a \"parquet\" value in the accept header and _outputFormat parameter. </p>\n</blockquote>\n<p>I think <em>just</em> the <code>_outputFormat</code> would change, not the <code>Accept</code> header, since the manifest of results would still be a JSON file with <code>output</code> entries.</p>\n</blockquote>\n<p>Certainly. The <code>Accept</code> header specifies the format of the \"<em>OperationOutcome resource response to the kick-off request</em>\", I've missed that. The spec's \"<em>Currently, only application/fhir+json is supported.</em>\" shouldn't change then.</p>",
        "id": 183034103,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1575960994
    },
    {
        "content": "<p>Hi all, here go the results of Spark/Parquet/NDJSON experiments, that cover the \"general purpose export\" (GPE) usecase described above. </p>\n<p>Regarding the \"analytics-friendly export\" (AFE) usecase - I have received information from the Apache Spark PMC team that the upcoming Spark versions will include significant improvements in support for nested columns - meaning that analytic workloads will run faster with files produced by GPE, eliminating some inefficiencies in columnar projection and predicate pushdown. The AFE approach will still be the most optimal for analytics - but, given the GPE improvements, and the fact that it doesnt require changes in FHIR Export specification other than allowing for a \"parquet\" value in the <code>_outputFormat</code> parameter (and a potential addition of a <code>_compressionType</code> parameter) - it makes sense to start with GPE. The AFE approach, that requires significant additions in the FHIR specification, will be considered at a later stage when we have more experience with Parquet GPE in FHIR.</p>\n<p>Ok, now for the GPE results. Per the discussion earlier in this thread, the goals of the experiments were to 1. show that Parquet can handle complex data types (with <code>contained</code> and <code>component</code> elements, and Questionnaire types with various depth levels) 2. compare the size of Bulk Export data in NDJSON and Parquet formats. 3. compare SQL query execution time on NDJSON and Parquet files</p>\n<p>This post contains a summary of these experiments. The detailed results can be found here.<br>\n<a href=\"https://docs.google.com/document/d/1zk0YJ8MVUqBoJ0OK6Gb1lHE1shYIl3xJ1hTl1hxW3pg/edit?usp=sharing\" target=\"_blank\" title=\"https://docs.google.com/document/d/1zk0YJ8MVUqBoJ0OK6Gb1lHE1shYIl3xJ1hTl1hxW3pg/edit?usp=sharing\">https://docs.google.com/document/d/1zk0YJ8MVUqBoJ0OK6Gb1lHE1shYIl3xJ1hTl1hxW3pg/edit?usp=sharing</a></p>\n<p><span class=\"user-mention\" data-user-id=\"191315\">@Josh Mandel</span> , please let me know if this is sufficient for addition of the Parquet option to the next FHIR Export specification draft, for validation by the community.</p>\n<p>The experiments were performed on a data derived from samples in <a href=\"https://www.hl7.org/fhir/downloads.html\" target=\"_blank\" title=\"https://www.hl7.org/fhir/downloads.html\">https://www.hl7.org/fhir/downloads.html</a> </p>\n<ol>\n<li>\n<p>For any type of resource data (Patients; Observations with and without <code>contained</code>, <code>component</code> elements; Questioinnaire with item depth ranging from 1 to 5, some with <code>contained</code> elements), Parquet files have correct schema, and the contents is fully identical to the original NDJSON files (both schema and data are the same).</p>\n</li>\n<li>\n<p>Parquet files are much smaller than NDJSON files with the same data. <br>\nPatient files are ~6 times smaller.<br>\nObservation files are ~40 times smaller.  <br>\nThe reason for stronger reduction in 'observations' is that they are less diverse than 'patients' - typically, only a few elements change from record to record (such as <code>id</code>, <code>value</code>) while the rest (<code>coding</code>, <code>performer</code>, <code>subject</code>) change slowly or don't change at all. This allows the Parquet 'dictionary encoding' mechanism to store the repeating fields using only their reference ids.</p>\n</li>\n<li>\n<p>SQL queries run significantly faster on Parquet files than on NDJSON files.</p>\n</li>\n</ol>",
        "id": 186183104,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1579614022
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"197632\">@Gidon Gershinsky</span> Are the query results in your report the sum of the time to load the file and the time to run the query? Or is the data loaded and cached in memory, but the queries run after take different time going over what should be equivalent data?</p>\n<p>I'm assuming it's the former, but wanted to be sure.</p>",
        "id": 188237582,
        "sender_full_name": "Matt Sargent",
        "timestamp": 1581708281
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"261034\">@Matt Sargent</span> Correct, its the former (even though the files are likely loaded from the host memory; the OS would optimize the disk reading). Still, as mentioned in the post, in many real-life use cases the files won't be co-located with the Spark worker host - instead, they would be stored in a distributed file system or in an object store. Then the Parquet in-storage filtering capabilities (columnar projection and predicate push-down) will further increase the relative difference in query execution time.  On the other hand, caching the NDJSON files in Spark memory will decrease the difference - at a price of fetching the full NDJSON datasets from the storage and keeping them in memory. Parquet files are not only smaller (for the same data), but allow for fetching a data subset.</p>",
        "id": 188333015,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1581875204
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"197632\">@Gidon Gershinsky</span>  for the analysis above! I haven't had much time to dig into this technology, but I suspect that as more implementations of the bulk data API come online and people are trying to actually use bulk export files for analysis as well as managing storage, there will be a natural gravitation to the kinds of tools you're exploring. <span class=\"user-mention\" data-user-id=\"196806\">@Edward Yurcisin</span> was describing to me just last week some of the challenges of working with larger export files.</p>\n<p>As a practical matter, I'd note that demonstrations and easy-to-use tooling can speak volumes. For example, if you thought about building an open source client that could pull JSON LD data from a server and then automatically convert them into parquet, demonstrating how to run queries against them efficiently -- this plus comparison benchmarks might significantly help bring people along.</p>",
        "id": 189707210,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1583338809
    },
    {
        "content": "<p>Hi Josh, sounds good! We do work on such technologies, and plan to open source them. We'll get back with that, including the benchmark results.</p>",
        "id": 189713749,
        "sender_full_name": "Gidon Gershinsky",
        "timestamp": 1583342596
    }
]