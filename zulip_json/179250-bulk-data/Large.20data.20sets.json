[
    {
        "content": "<p>I'm curious what the community considers a large data set to be? For example, if exporting patient data, would we consider 100,000 patients large? 500,000? a million? At what threshold does bulk fhir begin to break down because either there are a too many files, too many records per file, or the time it takes to generate the files in response to a kick off request too long to be manageable?</p>",
        "id": 197592543,
        "sender_full_name": "Bryan Schofield",
        "timestamp": 1589478620
    },
    {
        "content": "<p>I think this is specifically targeted at \"big data\" - so it's explicitly beyond the \"norms\" of how we likely handle data today (unless you're a big data system)</p>",
        "id": 197592786,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589478732
    },
    {
        "content": "<p>some of our initial \"very small\" sets of testing data was 10s of Gigs</p>",
        "id": 197592821,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589478754
    },
    {
        "content": "<p>(which doesn't work well on hotel wifi, at most of our connectathon events ;) )</p>",
        "id": 197592875,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589478784
    },
    {
        "content": "<p>how many records would you guess 10G to be?</p>",
        "id": 197592977,
        "sender_full_name": "Bryan Schofield",
        "timestamp": 1589478825
    },
    {
        "content": "<p>I think patient list size comes into it, but really you would be looking at the size of the record per patient</p>",
        "id": 197592986,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589478828
    },
    {
        "content": "<p>interesting... our use case so far hasn't been _per patient_, it's been _all of it_.</p>",
        "id": 197593143,
        "sender_full_name": "Bryan Schofield",
        "timestamp": 1589478884
    },
    {
        "content": "<p>I've tested it with about 400 million resources. Bulk-data itself works just fine, as long as everything is streaming. The hard part is to design your back-end and DB to handle such volumes properly.</p>",
        "id": 197593367,
        "sender_full_name": "Vladimir Ignatov",
        "timestamp": 1589478971
    },
    {
        "content": "<p>One billion resources is a large data set, enough to stress the parallelization of every part of a system. Resources seem to average around the ballpark of 1KB so that's 1TB on disk. Number of resources per patient varies wildly depending on the application.</p>",
        "id": 197593720,
        "sender_full_name": "Paul Church",
        "timestamp": 1589479088
    },
    {
        "content": "<p>well, from data IRL :)</p>",
        "id": 197593774,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589479118
    },
    {
        "content": "<p>We have large clients that have populations in the millions of patients and single EHR record sets in the 100s of tb</p>",
        "id": 197593826,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589479147
    },
    {
        "content": "<p>We're well into the billions. Observations alone are over 1.5B.</p>",
        "id": 197593897,
        "sender_full_name": "Bryan Schofield",
        "timestamp": 1589479179
    },
    {
        "content": "<p>that's a single health system</p>",
        "id": 197593898,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589479180
    },
    {
        "content": "<p>Any recommendations on balancing number of files with records per file? Or do consumers just assume they may be downloading hundreds or thousands of files?</p>",
        "id": 197594220,
        "sender_full_name": "Bryan Schofield",
        "timestamp": 1589479318
    },
    {
        "content": "<p>We limit the number of resources per file, currently set at 50k/file</p>",
        "id": 197594575,
        "sender_full_name": "Branden Rauch - CareEvolution",
        "timestamp": 1589479446
    },
    {
        "content": "<p>We consolidate the output into one file per resource type no matter how large they get.</p>",
        "id": 197594871,
        "sender_full_name": "Paul Church",
        "timestamp": 1589479563
    },
    {
        "content": "<p>That might depend on your implementation. For example can you handle 1B results from single SQL query, or do you prefer to split that over smaller \"pages\"... The clients should work with any size, unless you reach some OS file size limit.</p>",
        "id": 197595565,
        "sender_full_name": "Vladimir Ignatov",
        "timestamp": 1589479884
    },
    {
        "content": "<p>fascinating ... at 50k records, I'd be looking at, say 60,000 files, but unlimited records would yield a file that is 1.5TB. Both seem extreme for a client, but perhaps this within normal operating bounds</p>",
        "id": 197595816,
        "sender_full_name": "Bryan Schofield",
        "timestamp": 1589480000
    },
    {
        "content": "<p>most of the \"export it all\" would have to be from big data system to big data system, over pretty noice internet connections :)</p>",
        "id": 197596178,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589480179
    },
    {
        "content": "<p>Having 60,000 files looks worst to me than having single 1.5TB file (which shouldn't be extreme)</p>",
        "id": 197596274,
        "sender_full_name": "Vladimir Ignatov",
        "timestamp": 1589480224
    },
    {
        "content": "<p>Given how the files are individually listed in the bulk data status response once finished, it would be a mess to have 60,000 files.</p>",
        "id": 197596295,
        "sender_full_name": "Paul Church",
        "timestamp": 1589480236
    },
    {
        "content": "<p>We've talked about using chunking for the file transfers since these are so large (freeish in http/2, sometimes you have to do work for 1.1)</p>",
        "id": 197596331,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589480254
    },
    {
        "content": "<p>You also need to make sure your timeouts etc are going to handle how long this will take :)</p>",
        "id": 197596483,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589480311
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"197072\">@Paul Church</span>  it's not so much a mess since it's not a human reading it - it's a system</p>",
        "id": 197596533,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589480342
    },
    {
        "content": "<p>the system already has to handle many files</p>",
        "id": 197596584,
        "sender_full_name": "Jenni Syed",
        "timestamp": 1589480364
    },
    {
        "content": "<p>The bulk data responses are meant to be consumed by a computer service, not a human. Dealing with lots of files versus a few shouldn't make much difference. What if network issues prevent downloading a 1.5TB file?</p>",
        "id": 197596592,
        "sender_full_name": "Branden Rauch - CareEvolution",
        "timestamp": 1589480366
    },
    {
        "content": "<p>Network issues would concern me with large files.</p>",
        "id": 197596728,
        "sender_full_name": "Bryan Schofield",
        "timestamp": 1589480410
    },
    {
        "content": "<p>50k resources a file was a totally arbitrary number we came up with. It's better suited to resources that contain a lot of data as opposed to \"thinner\" resources like Observation. In the long run I think we will limit the number of resources per file depending on the resource type.</p>",
        "id": 197596817,
        "sender_full_name": "Branden Rauch - CareEvolution",
        "timestamp": 1589480452
    },
    {
        "content": "<p>Use a file size limit instead of a # of resources limit?</p>",
        "id": 197597076,
        "sender_full_name": "Michele Mottini",
        "timestamp": 1589480560
    },
    {
        "content": "<p>It's just that the API response to the status request is usually pretty small, but with many files there's no bound on it and it's not paginated or anything.</p>",
        "id": 197597090,
        "sender_full_name": "Paul Church",
        "timestamp": 1589480570
    },
    {
        "content": "<p>Resumable downloads are doable but could be hard to implement. The easiest option for the clients seems to be to retry certain number of times.</p>",
        "id": 197597928,
        "sender_full_name": "Vladimir Ignatov",
        "timestamp": 1589480917
    },
    {
        "content": "<p>does BITS protocol help here and how to add that in the implementation ? <span class=\"user-mention\" data-user-id=\"191315\">@Josh Mandel</span></p>",
        "id": 222915727,
        "sender_full_name": "Ritika Jain",
        "timestamp": 1610740484
    },
    {
        "content": "<p>I'm not sure what this is; can you share a link or specific details about what you have in mind?</p>",
        "id": 222917395,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1610741135
    },
    {
        "content": "<p>BITS transfers files between machines using idle network bandwidth. I am wondering if something like this could be incorporated since downloading multiple files ( say 60k files) or huge size of data  ( in TBs ) both could be a problem</p>",
        "id": 222919138,
        "sender_full_name": "Ritika Jain",
        "timestamp": 1610741830
    },
    {
        "content": "<p>My framework here is: the spec allows for extensions that can explore these sorts of capabilities; I'd look to specific real-world performance issues here before we try to optimize in a first-class, \"in-spec\" way.</p>",
        "id": 222919507,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1610742000
    },
    {
        "content": "<p>The major concern around bulk data export is exporting a large chunk of data especially when we talk about system-level export. </p>\n<p>Any suggestions on how to handle the system level export and also meet the ONC compliance guidelines?</p>",
        "id": 228368353,
        "sender_full_name": "Aditya Chhabra",
        "timestamp": 1614671972
    },
    {
        "content": "<p>I'm not sure what level of suggestion you're thinking about. In general the ONC requirement is to support Bulk Data Export for a group of patients.</p>",
        "id": 228438848,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1614702245
    }
]