[
    {
        "content": "<p>I'm chatting with a vendor about uploading bulk data. They would like to know how we would recommend uploading the results of a bulk data query to their own FHIR server .</p>",
        "id": 159398392,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551157007
    },
    {
        "content": "<p>here's my thoughts:</p>",
        "id": 159398394,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551157011
    },
    {
        "content": "<p>(we're also planning to define an $import operation this spring)</p>",
        "id": 159398425,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551157061
    },
    {
        "content": "<p>Would love your thoughts!</p>",
        "id": 159398443,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551157081
    },
    {
        "content": "<p>Post the nd-json files, one at a time to [base], the base URL for the service, with a mime type of application/fhir+ndjson, and the server automatically treats this is a batch operation to upload the resources in the nd-json file</p>",
        "id": 159398490,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551157095
    },
    {
        "content": "<p>this is less than the $import operation we've previously talked about, but very simple to get going with.</p>",
        "id": 159398496,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551157118
    },
    {
        "content": "<p>If we're talking about net new server behavior, might be more robust to post something like the full status response so the server can process files in the background ... But sure, if processing full ndjson files synchronously is feasible for this server, I'd say go for it.</p>",
        "id": 159398559,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551157204
    },
    {
        "content": "<p>How would that work if there are references between some of the resources - e.g. a Patient pointing to Organization</p>",
        "id": 159398597,
        "sender_full_name": "Lloyd McKenzie",
        "timestamp": 1551157281
    },
    {
        "content": "<p>Uploading large files can be a pain because (as far as I know) there's no good standardized resume approach. (Is there?)</p>",
        "id": 159398605,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551157298
    },
    {
        "content": "<p>upload - would have to accept broken links. resume - not catering for that in this simple API</p>",
        "id": 159398653,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551157326
    },
    {
        "content": "<p>Accept them, sure.  But would it repair them as other resources are loaded?</p>",
        "id": 159398667,
        "sender_full_name": "Lloyd McKenzie",
        "timestamp": 1551157383
    },
    {
        "content": "<p>If it keeps the original ids, that's easy.  But if it changes them, there'd be update work involved</p>",
        "id": 159398677,
        "sender_full_name": "Lloyd McKenzie",
        "timestamp": 1551157420
    },
    {
        "content": "<p>Makes sense. You could also just have the client turn ndjson files into bundles (can be done in a streaming fashion even) so the server doesn't need special behavior</p>",
        "id": 159398679,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551157424
    },
    {
        "content": "<p>If synchronous upload and one file at a time and suspended referential integrity checks are all okay.</p>",
        "id": 159398721,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551157448
    },
    {
        "content": "<blockquote>\n<p>you could also just have the client turn ndjson files into bundles</p>\n</blockquote>\n<p>yes, you could. Not really consistent with our general philosophy of shoving work to the server where it's convenient</p>",
        "id": 159401142,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551160931
    },
    {
        "content": "<p>Could we post the export status result, then let the server manage the rest?</p>",
        "id": 159407133,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1551169212
    },
    {
        "content": "<p>Grabbing the large binaries as it can.</p>",
        "id": 159407208,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1551169316
    },
    {
        "content": "<p>Not a fan of the post ndjson to the root... The size of these files is likely larger than post can handle.</p>",
        "id": 159407282,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1551169391
    },
    {
        "content": "<p>And status and delete processing could work the same as export.</p>",
        "id": 159407389,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1551169541
    },
    {
        "content": "<p>My research in uploading large files was dissapointing. There is no good way (most of services like dropbox, s3 just split and upload  in chunks). Even streaming http working only for download. We can turn upload into download pointing server to download api?</p>",
        "id": 159407457,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551169602
    },
    {
        "content": "<p>My suggestion was to post the final output of the export to the import.</p>",
        "id": 159407550,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1551169749
    },
    {
        "content": "<p>Another option is use web sockets channel  - but it does not look resty :)</p>",
        "id": 159408012,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551170263
    },
    {
        "content": "<p>What’s the limit on post size? I don’t have any limits....</p>",
        "id": 159425745,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551189819
    },
    {
        "content": "<blockquote>\n<blockquote>\n<p>you could also just have the client turn ndjson files into bundles</p>\n</blockquote>\n<p>yes, you could. Not really consistent with our general philosophy of shoving work to the server where it's convenient</p>\n</blockquote>\n<p>The context for this discussion was, I thought, a simple stopgap -- before we define an import operation. But if your client only needs to work with one server and that server developer is willing to do the work, then by all means :-)</p>",
        "id": 159425998,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551190001
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> it is usually limited by your web server implementation, memory buffers and disk cache and all proxies on the way. In google cloud or aws if i’m not wrong - max size is about 4g, but real chunks sent by official sdk are much smaller.</p>",
        "id": 159443516,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551204024
    },
    {
        "content": "<p>For downloading you can use <a href=\"https://ru.wikipedia.org/wiki/Chunked_transfer_encoding\" target=\"_blank\" title=\"https://ru.wikipedia.org/wiki/Chunked_transfer_encoding\">https://ru.wikipedia.org/wiki/Chunked_transfer_encoding</a> and be memory efficient with ndjson processing data in a stream.</p>",
        "id": 159443920,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551204394
    },
    {
        "content": "<p>But this protocol almost not supported by HTTP client libraries for uploading.</p>",
        "id": 159444077,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551204543
    },
    {
        "content": "<p>Potentially we can open web socket and stream data with back pressure and some type of cursor to restore upload in a specific position, but this is not REST at all :(</p>",
        "id": 159444191,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551204643
    },
    {
        "content": "<p>Databases replicate data thro socket connection or by producing  chunks of replication log as small files indexed by counter - <a href=\"/user_uploads/10155/Q7ItgcAJ853wp4m6YTlvAOj6/pasted_image.png\" target=\"_blank\" title=\"pasted_image.png\">pasted image</a> <a href=\"http://1.bp.blogspot.com/_26KnjtB2MFo/SYVDrEr1HXI/AAAAAAAAAEY/ncq_AW-Vv-w/s320/pg_warm_standby.png\" target=\"_blank\" title=\"http://1.bp.blogspot.com/_26KnjtB2MFo/SYVDrEr1HXI/AAAAAAAAAEY/ncq_AW-Vv-w/s320/pg_warm_standby.png\">http://1.bp.blogspot.com/_26KnjtB2MFo/SYVDrEr1HXI/AAAAAAAAAEY/ncq_AW-Vv-w/s320/pg_warm_standby.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/10155/Q7ItgcAJ853wp4m6YTlvAOj6/pasted_image.png\" target=\"_blank\" title=\"pasted image\"><img src=\"/user_uploads/10155/Q7ItgcAJ853wp4m6YTlvAOj6/pasted_image.png\"></a></div><div class=\"message_inline_image\"><a href=\"http://1.bp.blogspot.com/_26KnjtB2MFo/SYVDrEr1HXI/AAAAAAAAAEY/ncq_AW-Vv-w/s320/pg_warm_standby.png\" target=\"_blank\" title=\"http://1.bp.blogspot.com/_26KnjtB2MFo/SYVDrEr1HXI/AAAAAAAAAEY/ncq_AW-Vv-w/s320/pg_warm_standby.png\"><img src=\"https://uploads.zulipusercontent.net/af805a1b2e8d51dea75a5cd54d09fbb392667eb3/687474703a2f2f312e62702e626c6f6773706f742e636f6d2f5f32364b6e6a7442324d466f2f53595644724572314858492f41414141414141414145592f6e63715f41572d56762d772f733332302f70675f7761726d5f7374616e6462792e706e67\"></a></div>",
        "id": 159444371,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551204803
    },
    {
        "content": "<p><a href=\"https://gist.github.com/niquola/2bf49e7c9119c425261389e3a7a3d1d3\" target=\"_blank\" title=\"https://gist.github.com/niquola/2bf49e7c9119c425261389e3a7a3d1d3\">https://gist.github.com/niquola/2bf49e7c9119c425261389e3a7a3d1d3</a> here is my notes from amsterdam discussion</p>",
        "id": 159444609,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1551205008
    },
    {
        "content": "<p>Usually a setting in iis, and default isn't that big, nowhere near gigabytes that I've experienced. That's where compression denial of service attacks go. I'll try dig out the setting I use in .Net to make it bigger (to 4mb I think I made it so larger codesystems can be loaded)</p>",
        "id": 159452130,
        "sender_full_name": "Brian Postlethwaite",
        "timestamp": 1551210589
    },
    {
        "content": "<p>In my opinion uploading the files directly through the API doesn't seem practical except in use cases where the amount of data is very small. In those cases batch bundles seem to be a better option. Giving the server something like the result of an export (with a list of files) seems more practical.</p>",
        "id": 159477529,
        "sender_full_name": "Michael Hansen",
        "timestamp": 1551235507
    },
    {
        "content": "<p>indeed. but we were looking for a stop gap measure</p>",
        "id": 159861807,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1551642043
    },
    {
        "content": "<p>(for stop gap, I still think I'd  just have the client produce a normal, compliant bundle)</p>",
        "id": 159862757,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1551643716
    },
    {
        "content": "<p>We have been experimenting with bulk import in Google Cloud quite a bit. I'll start a separate thread with some lessons learned for the general case. For the stopgap case, you can do a lot with batch/transaction bundles if the client does the preprocessing.</p>\n<p>We wrote some client code that walks a set of input files, parses resources, constructs the reference graph, does reference tiering, and produces a sequence of transaction bundles that can be executed in sequential order to accomplish the import while preserving referential integrity at every point. There is some risk that the data could have reference cycles too large to post as a bundle and/or to execute in a single transaction, but that's pretty unlikely in our experience. In practice this has been quite successful at getting data in, at the cost of some heavy lifting on the client side.</p>\n<p>I'm bringing this up partly just to agree with what has already been said above, but also because referential integrity and bundle semantics are a major challenge in defining the general $import operation.</p>",
        "id": 161090651,
        "sender_full_name": "Paul Church",
        "timestamp": 1552938974
    },
    {
        "content": "<p>how have you handled circular dependencies?</p>",
        "id": 161091079,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1552939275
    },
    {
        "content": "<p>Put the whole cycle in a single bundle. As part of the transaction, the bundle's local references will either remain intact (for PUT) or get rewritten to server-assigned IDs (for POST).</p>",
        "id": 161096680,
        "sender_full_name": "Paul Church",
        "timestamp": 1552943334
    }
]