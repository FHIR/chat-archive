[
    {
        "content": "<p>Is there more information about what the flat-file export format might be relevant to <a href=\"http://docs.smarthealthit.org/flat-fhir/\" target=\"_blank\" title=\"http://docs.smarthealthit.org/flat-fhir/\">http://docs.smarthealthit.org/flat-fhir/</a>?</p>\n<p>We've started using NDJSON formatted content for a number of our bulk export formats (<a href=\"http://ndjson.org/\" target=\"_blank\" title=\"http://ndjson.org/\">http://ndjson.org/</a>), the same as what Elastic Search uses so we can actually stream out the data from the system without any significant memory burden on our servers.</p>\n<p>Where would we comment on what the export format might end up being?</p>",
        "id": 153902661,
        "sender_full_name": "David Teirney",
        "timestamp": 1502918844
    },
    {
        "content": "<p>I was initially thinking of FHIR bundles as the output format, but agree that newline delimited JSON seems like a better fit for large datasets. In general, we  should probably use github issues to manage changes to the spec, but since it's at such an early stage I went ahead and just updated the proposal.</p>",
        "id": 153902694,
        "sender_full_name": "Dan Gottlieb",
        "timestamp": 1502975580
    },
    {
        "content": "<p>So right now, it's just a proposal (the export part I mean?) It's just that it's inclusion in an RFI would suggest that it's a bit more mature than that...</p>",
        "id": 153902723,
        "sender_full_name": "David Hay",
        "timestamp": 1503004012
    },
    {
        "content": "<p>Yup, just a proposal and not very mature. Also, to clarify, it's not in the published model RFP language, just in the optional add-on functionality section in the draft langauge we posted for comment as a possible method of specifying bulk data export. That said, any thoughts on the best way to make it more clear on the flat fhir page that it's still a very early stage proposal?</p>",
        "id": 153902761,
        "sender_full_name": "Dan Gottlieb",
        "timestamp": 1503061887
    },
    {
        "content": "<p>using ndjson doesn't begin to cover the kind of use cases we have been talking about</p>",
        "id": 153902822,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1503098728
    },
    {
        "content": "<p>Interesting - what's an example of such a use case?</p>",
        "id": 153902871,
        "sender_full_name": "Dan Gottlieb",
        "timestamp": 1503169067
    },
    {
        "content": "<p>see the analytics on FHIR stream, and look for mention of Parquet</p>",
        "id": 153902942,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1503297808
    },
    {
        "content": "<p>On flat file export: it might still be helpful to think about patient-oriented exports that leverage <code>$everything</code> (which could mean newline-delimited JSON where each line is a whole patient-specific $everything bundle)</p>",
        "id": 153903012,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1503333022
    },
    {
        "content": "<p>Thanks Grahame - so the use case is to have the data in a format that can be used natively in the Hadoop ecosystem? I don't have big data experience, so this may be a naive question, but how critical is this relative to exporting in ndjson and converting for use in Hadoop? It seems like going directly to a binary format like Parqet will complicate things on the FHIR server side (which can already serialize json), and preclude a bunch of other use cases (like easily loading a small or mid-sized dataset into mysql).</p>",
        "id": 153903613,
        "sender_full_name": "Dan Gottlieb",
        "timestamp": 1503604714
    },
    {
        "content": "<p>Thatâ€™s a good point Josh. One file per patient has the nice property of making it simple to filter patients and only ETL a subset, though it may result in a lot of files with just a few resources in each for a nightly update. As you pointed out, in either case, the ndjson format would work and has the nice property of being parallelizable in both the generation and ETL stages.</p>",
        "id": 153903614,
        "sender_full_name": "Dan Gottlieb",
        "timestamp": 1503605005
    },
    {
        "content": "<p>that's not a definition of parallelizable I'm familiar with</p>",
        "id": 153903624,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1503609017
    },
    {
        "content": "<p>I don't think hadoop itself is a requirement, but the approach should be scalable.  to me, ndjson isn't really - you still need to parse json</p>",
        "id": 153903625,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1503609072
    },
    {
        "content": "<p>Why isn't parsing json scalable? A binary format would be smaller and faster, but json has the benefit of simplicity. Is the performance penalty really too big to use it as an intermediate data format?</p>",
        "id": 153903629,
        "sender_full_name": "Dan Gottlieb",
        "timestamp": 1503611013
    },
    {
        "content": "<p>it's certainly simpler. But there's no answer to the second question - the only way to find out is to try in a variety of contexts</p>",
        "id": 153903630,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1503611127
    }
]