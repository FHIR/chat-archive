[
    {
        "content": "<p>Not sure if this is the proper forum, but has anyone done any work using a more traditional relational model, and something like a Postgres XL?    <a href=\"http://www.postgres-xl.org/\" target=\"_blank\" title=\"http://www.postgres-xl.org/\">http://www.postgres-xl.org/</a></p>",
        "id": 153891943,
        "sender_full_name": "Craig McClendon",
        "timestamp": 1496329943
    },
    {
        "content": "<p>@<strong>nicola (RIO)</strong> uses Postgresql and is well connected to the dev team</p>",
        "id": 153891974,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1496349947
    },
    {
        "content": "<p>We use a mix of postgres and nosql in ontoserver</p>",
        "id": 153892002,
        "sender_full_name": "Jim Steel",
        "timestamp": 1496362321
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"193261\">@craig mcclendon</span> we use postgresql with jsonb - so it's like hybrid approach between document dbs and relational dbs. We initially tried to generate pure relational model - but it's about &gt; 1K tables and joins are slow. Here is slides from my Madrid talk - <a href=\"https://niquola.github.io/madrid-hl7-2017-slides/slides/index.html#/2\" target=\"_blank\" title=\"https://niquola.github.io/madrid-hl7-2017-slides/slides/index.html#/2\">https://niquola.github.io/madrid-hl7-2017-slides/slides/index.html#/2</a>. PostgresXL is little bit outdated (as i remember they are on 9.4 version), but i know guys from postgrespro company who are working to update of PostgresXL to latest pg version. There are also greenplum and citusdb extensions - promising for pg analytic.</p>",
        "id": 153892062,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1496394586
    },
    {
        "content": "<p>Thx for link to PostGres-XL.  I've been neglecting that project.  I've also been tinkering with Apache Accumulo (<a href=\"https://accumulo.apache.org/\" target=\"_blank\" title=\"https://accumulo.apache.org/\">https://accumulo.apache.org/</a>) as primary backing store but still noodling about right 'granularity' for the data, if that makes sense.  Cheers.</p>",
        "id": 153892151,
        "sender_full_name": "Gary Teichrow",
        "timestamp": 1496425316
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"191318\">@nicola (RIO/SS)</span>  I am thinking to convert 10-15 different resources  we are getting from vendor into relational database. Was 1K tables in your case covering all 100+ FHIR resources? The problem with storing FHIR data as JSON  objects is that traditional BI tools cannot work with nested structures well and also we want to hide complexity of FHIR resources for our BI Developers.</p>",
        "id": 153922670,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512229350
    },
    {
        "content": "<p>Hi, usually in bi you write direct sql, so with pg this is not a problem. Another solution is to create flatten views in the same database</p>",
        "id": 153922687,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1512246798
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191318\">@nicola (RIO/SS)</span> right you can use SQL and dotted json notation and most RDBMS support json as an object these days (Oracle, MS SQL, Postgres) but it is still not a trivial task to do so because of nested arrays and references between documents. You had a nice slide about these challenges. I guess I am asking what everyone else are doing with FHIR documents if they need to analyze data and build dashboards using BI tools like QlikView, Tableau, Oracle BI etc. I was going to normalize all the nested structures (I believe I can generate DDLs and logic) but before I spend my time on it, wanted to see how everyone else is doing this</p>",
        "id": 153922689,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512252225
    },
    {
        "content": "<p>no one has shared anything about this, though there are people doing it. But everyone has different styles of use of database (e.g. how much to normalise), and different use of underlying technologies, and different compromises due to additional constraints in the underlying data. As well as additional considerations around terminology infrastructure</p>",
        "id": 153922723,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1512339013
    },
    {
        "content": "<p>all of this means that there's not a lot of mileage in sharing DDL scripts.</p>",
        "id": 153922724,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1512339033
    },
    {
        "content": "<p>also, as you have found, non-transactional systems (for analysis/reporting) are starting to move to json based tooling and just using the resources directly.</p>",
        "id": 153922725,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1512339081
    },
    {
        "content": "<p>Thanks <span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> makes sense to me. I think I am going to try a hybrid approach - store bundle resources as json values and expose some of the key data using normalized version for BI Developers. That way we have FHIR native data available for analytics and we have easier to use normalized tables for dashboards and such.</p>",
        "id": 153922728,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512343883
    },
    {
        "content": "<p>found two interesting publications, focused on conversion of semi-structured json to relational model.</p>\n<p><a href=\"http://cs-www.cs.yale.edu/homes/dna/papers/schemagen-sigmod16.pdf\" target=\"_blank\" title=\"http://cs-www.cs.yale.edu/homes/dna/papers/schemagen-sigmod16.pdf\">http://cs-www.cs.yale.edu/homes/dna/papers/schemagen-sigmod16.pdf</a></p>\n<p><a href=\"http://digitalcommons.unf.edu/cgi/viewcontent.cgi?article=1781&amp;context=etd\" target=\"_blank\" title=\"http://digitalcommons.unf.edu/cgi/viewcontent.cgi?article=1781&amp;context=etd\">http://digitalcommons.unf.edu/cgi/viewcontent.cgi?article=1781&amp;context=etd</a></p>",
        "id": 153922804,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512407465
    },
    {
        "content": "<p>someone pointed me to an excellent write-up about EAV modeling concepts on wiki <a href=\"https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model\" target=\"_blank\" title=\"https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model\">https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model</a>. It is a long article but it explains different options on how to deal with columnar representation of EAV data for statistical / analytical tools.</p>",
        "id": 153923922,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512841191
    },
    {
        "content": "<p>Another approach I've been thinking of is more solution bound: Vonk on SQL Server indexes all the search parameter values into a couple of simple tables that should be a fairly good source for star schemas. It can also process custom search parameters, with custom fhirpath expressions in them - so you can get those in the index as well.</p>",
        "id": 153924724,
        "sender_full_name": "Christiaan Knaap",
        "timestamp": 1513167287
    },
    {
        "content": "<p>Here was a simple example I created just to get a sense of what it might look like to model out the resources and datatypes and what the queries might look like. <br>\nIt's using a generic table \"relationship_mapper\" to manage the relationships between objects. </p>\n<p>In this case I was just looking at how to represent and query Observation.code. <br>\n(this is psql syntax)</p>\n<hr>\n<p>CREATE TABLE Observation (<br>\npkey text, <br>\n_status text<br>\n);</p>\n<p>CREATE TABLE CodeableConcept(<br>\n  pkey text, <br>\n  _text text<br>\n);</p>\n<p>CREATE Table Coding (<br>\n    pkey text,<br>\n    _system text, <br>\n    _version text, <br>\n    _code text, <br>\n    _display text,<br>\n    _userSelected boolean<br>\n);</p>\n<p>-- generic relationship mapping table<br>\nCREATE TABLE relationship_mapper (<br>\n    pkey text, <br>\n    table_a text, <br>\n    table_b text,<br>\n    table_a_key text,<br>\n    table_b_key text,<br>\n    relationship_type text<br>\n);</p>\n<p>-- insert an observation with a code<br>\nINSERT INTO Observation (pkey) VALUES ('OBS1');<br>\nINSERT INTO CodeableConcept(pkey) VALUES ('CC1');<br>\nINSERT INTO Coding(pkey, _system, _code) VALUES ('CODING1', '<a href=\"http://loinc.org\" target=\"_blank\" title=\"http://loinc.org\">http://loinc.org</a>', '8867-4');<br>\nINSERT INTO relationship_mapper VALUES ('R1', 'Observation', 'CodeableConcept', 'OBS1', 'CC1', 'code');<br>\nINSERT INTO relationship_mapper VALUES ('R2', 'CodeableConcept', 'Coding', 'CC1', 'CODING1', 'coding');</p>\n<p>drop view ObservationCode;<br>\nCREATE VIEW ObservationCode as<br>\nSELECT r1.table_a_key as ObservationKey, tab.*<br>\nFROM CodeableConcept tab<br>\nINNER JOIN relationship_mapper r1 ON (<br>\n    r1.table_a = 'Observation'<br>\n    AND r1.relationship_type = 'code'<br>\n    AND r1.table_b = 'CodeableConcept'<br>\n    AND r1.table_b_key = tab.pkey);</p>\n<p>drop view CodeableConcept2Coding;<br>\nCREATE VIEW CodeableConcept2Coding as<br>\nSELECT r1.table_a_key as CodeableConceptPkey, tab.*<br>\nFROM Coding tab<br>\nINNER JOIN relationship_mapper r1 ON (<br>\n    r1.table_a = 'CodeableConcept'<br>\n    AND r1.relationship_type = 'coding'<br>\n    AND r1.table_b = 'Coding'<br>\n    AND r1.table_b_key = tab.pkey);</p>\n<p>--SELECT * FROM ObservationCode<br>\n--SELECT * FROM CodeableConcept2Coding<br>\n--SELECT * FROM relationship_mapper</p>\n<p>-- find all observations with a particular code<br>\nSELECT *<br>\nFROM Observation obs<br>\nWHERE EXISTS (<br>\n        SELECT * <br>\n        FROM ObservationCode cc<br>\n        INNER JOIN CodeableConcept2Coding coding ON (cc.pkey = coding.CodeableConceptPkey)<br>\n        WHERE cc.ObservationKey = obs.pkey<br>\n          AND coding._system = '<a href=\"http://loinc.org\" target=\"_blank\" title=\"http://loinc.org\">http://loinc.org</a>'<br>\n          AND coding._code = '8867-4' )</p>",
        "id": 153926029,
        "sender_full_name": "Craig McClendon",
        "timestamp": 1513626914
    }
]