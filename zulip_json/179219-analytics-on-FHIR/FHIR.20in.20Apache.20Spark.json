[
    {
        "content": "<p>I thought this group might be interested in this. We've open sourced a library that can import FHIR resources as native Apache Spark data structures. The goal is to make it easy to do deep analysis of FHIR data with popular tools at scale. Details are here:</p>\n<p><a href=\"http://engineering.cerner.com/blog/announcing-bunsen-fhir-data-with-apache-spark/\" target=\"_blank\" title=\"http://engineering.cerner.com/blog/announcing-bunsen-fhir-data-with-apache-spark/\">http://engineering.cerner.com/blog/announcing-bunsen-fhir-data-with-apache-spark/</a></p>",
        "id": 153921704,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1511818979
    },
    {
        "content": "<p>I'm juggling a couple other things today, but will be checking on this stream for any questions, or if anyone wants to discuss the topic further.</p>",
        "id": 153921706,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1511819058
    },
    {
        "content": "<p>looks good - I'll be giving it a go some time in the nexts few weeks</p>",
        "id": 153921709,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1511822155
    },
    {
        "content": "<p>thanks</p>",
        "id": 153921710,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1511822158
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> Sounds good, and thanks. It's still early, but we plan on growing it based on emerging demands. R support and integration with emerging bulk data sharing specifications are likely early candidates.</p>",
        "id": 153921716,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1511822760
    },
    {
        "content": "<p>have you seen the R on FHIR Library that Furore and I have been working on?</p>",
        "id": 153921717,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1511824067
    },
    {
        "content": "<p>Yes, I read your posts on that. I think we might have complementary audiences here: There are R users looking for a good, straightforward way to tap into FHIR services, and also R users with a \"data lake\"-style environment looking to do analysis at a very large scale. That said, we'd be interested in harmonizing those efforts around any commonality that we find along the way.</p>",
        "id": 153921718,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1511824491
    },
    {
        "content": "<p>makes sense</p>",
        "id": 153921722,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1511825222
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span>  Cerner client here <span class=\"emoji emoji-1f44b\" title=\"wave\">:wave:</span> It is awesome to see  Cerner doing these things! Have your thought about normalizing FHIR nested structures into flat non-nested tables so one can use traditional BI tools that would not understand nested structures? I starred Bunsen's github project, will definitely check it out!</p>",
        "id": 153922668,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512227356
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span> my problem with Spark/Parquet approach is that it is not possible to make it real-time and also do updates/deletes as we are getting bundles from a third party vendor and they might resend the same bundle with changes more than once.  Impala and Hive support json nicely but I am thinking to use Kudu and normalize FHIR resources into flat tables like patient_names, patient_history etc. Kudu does not support nested structures (yet) but handles updates, deletes and random lookups nicely and still providing decent performance for analytical and BI type queries through Impala. Do you think it is a bad approach? Another option I have is to load Json as value to Oracle database but my concern there is scalability and performance</p>",
        "id": 153922690,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512252571
    },
    {
        "content": "<p>Thanks, <span class=\"user-mention\" data-user-id=\"194933\">@Boris Tyukin</span>! </p>\n<p>To answer your questions, we want to keep the nested structures by default because there isn't a clean way to flatten nested lists without losing data, and quite a few FHIR resources take advantage of this. However, if you want a flattened view, it's simple to write a Spark SQL query that transforms a nested structure into a flattened view that is appropriate for other tools. We actually do this quite a bit to create easily-consumed reports.  Also, it's convenient to use structures for passing to user-defined functions, like the in_valueset seen in the link above, so it can work with the full structure. </p>\n<p>That all said, we'd be open to adding \"flatten\" and \"expand\" functions to this system if there is some agreed upon convention for handling things like nested lists. </p>\n<p>Regarding Parquet: while we do use Parquet for our default format just because Spark , this system isn't tightly coupled to it. A Kafka stream of these messages, for example, could be used for realtime processing of data. I'm sure we could find lots of permutations for this sort of thing...we just happened to start with an analytic and research workload rather than a streaming one.</p>",
        "id": 153922864,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1512436553
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span> I like that! I am new to FHIR and the more I learn about FHIR models, the more I come to realize  that it does not make sense to normalize resources because of complexity of nested lists. My other concern that normalization rules will change from one FHIR version to another so I totally see your point. I know you use HBase a lot  over at Cerner - have you tried to store FHIR resources into HBase? I know it is not good for analytics but curious if you tried that.  </p>\n<p>As far as Bunsen, any plans to support  DSTU 2 soon? I think this is the version currently supported by Cerner Ignite. Our other vendor happened to be on DSTU 2 as well...</p>",
        "id": 153922881,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512444351
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"194933\">@Boris Tyukin</span> We haven't stored FHIR resources themselves in HBase, but we store similarly nested internal structures there frequently, so I'm certain it can be done.</p>\n<p>Also, I've logged the first issue to Bunsen, to add DSTU2 support if you want to track it here: <a href=\"https://github.com/cerner/bunsen/issues/1\" target=\"_blank\" title=\"https://github.com/cerner/bunsen/issues/1\">https://github.com/cerner/bunsen/issues/1</a>. We had started with DSTU3 on this since we knew we would need it no matter what, but I think we have clean approach that we can use for DSTU2 as well.</p>",
        "id": 153922895,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1512452361
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span> wanted to try it out with synthea generated data sets but looks like the library does not work with spark 2.2 which is what we have. I already saw a github issue opened. For now, I just ended up using native Spark's json processing but I think I understand now why you guys built Bunsen - too many lines of codes to get to FHIR data and model properly using FHIR schema. It was not clear to me at first why you built it.</p>",
        "id": 153923894,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512836312
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"194933\">@Boris Tyukin</span>  thanks for the report. Could you perhaps share some brief examples of the kinds of processing you're doing natively that's verbose/painful? It'd be nice to have some concrete examples to talk about.</p>",
        "id": 153923997,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1512879311
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191315\">@Josh Mandel</span> All I wanted to do is to take a bunch of FHIR bundles from our vendor and to load them to Hive/Impala, using parquet as a file format. Every bundle has 5-10 different resource types and depending on a use case (vendor supports a few around specific medical problems), resources can have different elements even for the same resource type.</p>\n<p>When I tried to load bundles as is to Spark, Spark would infer schema from JSON but the schema would include ALL the resource types elements so elements from a patient resource and an observation resource would mix and the schema will contain all of them. I solved that by parsing bundles first by resource types  and then storing these resources on HDFS separately (patient parquet files, observations parquet files etc.) This required one extra pre-processing step which is not optimal but I do not see a way around it.</p>\n<p>Next challenge was to do some SQL - even basic joins (e.g. join patient data with observation and encounter data) require good understanding of FHIR and the way identifiers work.</p>\n<p>But the main problem is that both our mainstream BI tools (Qlik and Oracle BI) do not quite work with nested data so I am thinking now to map some of the data stored in nested parquet files to flat columnar tables which is quite a task. I wanted to come up with some generic approach to do so, but realized it would not make a lot of sense because of highly nested structures, various versions of FHIR and the fact that a lot of FHIR resource elements are optional. But at the same time I do not expect our analysts to dig into FHIR specs every time they want to do simple queries against FHIR data - it is just too time consuming unless you deal with FHIR every day :)</p>",
        "id": 153924097,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512932938
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191315\">@Josh Mandel</span> just noticed you have this project on Github <a href=\"https://github.com/jmandel/synthea-to-bigquery\" target=\"_blank\" title=\"https://github.com/jmandel/synthea-to-bigquery\">https://github.com/jmandel/synthea-to-bigquery</a> - interesting, I will check it out later. Looks like you also ended up parsing resources out of bundles and I see quite a lot of code to generate schemas for resources. I was looking for a tool or library to generate schemas for FHIR resources for Avro or Parquet based on profiles but could not find one.</p>",
        "id": 153924105,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1512933513
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span> Where/how do you guys store FHIR bundles? In my case, it can grow to 100s of millions of FHIR bundles and I do not want them to store as individual json files on HDFS. I am considering document-based NoSQL database like MongoDB but prefer to use just Hive / Spark since most of the analytics on these bundles will be ad-hoc.</p>",
        "id": 153949480,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1522438379
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"194933\">@Boris Tyukin</span> Hi, my apologies, somehow I missed this message earlier.  For our workload we're not natively storing the FHIR bundles, but are generating them as needed from an internal data model in our Spark jobs...and then storing the resulting datasets as Parquet files via Bunsen for analysis. </p>\n<p>I have not really used MongoDB, but if your primary access pattern is Spark, then I'd recommend just storing them as rows in a Hive table. This will be the most efficient way to load them in the Hadoop ecosystem, and generally I like to try to keep everything we possibly can as a registered Hive table since it's easy to discover datasets and manage them from one place. Of course, this all depends on your access patterns, so your mileage may vary...</p>",
        "id": 153950992,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1522954576
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span>,  this is my thought exactly. With your recent commit to Bunsen, it is possible now. I do not really want to manage another database on our cluster and so far Hive/Spark/Parquet/Bunsen combo works really well. I am hoping to blog about it soon if you do not mind. </p>\n<p>BTW I have also looked at Apache Drill, but with Drill, one to have dump individual json files to an HDFS folder - in my case it is not going to work as we might end up with 100s millions of bundles once we are in production.</p>",
        "id": 153951044,
        "sender_full_name": "Boris Tyukin",
        "timestamp": 1522966213
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"194933\">@Boris Tyukin</span> Yes, agreed. And I would love to see a blog post on this topic. Please send it my way when it is up!</p>",
        "id": 153951061,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1522972486
    },
    {
        "content": "<p>I think <span class=\"user-mention\" data-user-id=\"191351\">@Chris Grenz</span> has experience with Apache Drill with lots of resources.</p>",
        "id": 153951227,
        "sender_full_name": "Christiaan Knaap",
        "timestamp": 1523039419
    },
    {
        "content": "<p>Indeed I do.  Accessing resources with Drill in Bundles is possible, but some inconsistencies in schema between resources can cause it to be tricky at best.  It's probably adequate for exploration.</p>",
        "id": 153951388,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1523210861
    },
    {
        "content": "<p>One nice thing about Drill is the ability to create parquet files based on a query. A strategy that may work well is to leave the FHIR bundles as-is and use Drill to extract the set interesting to you to create parquet files for purpose. Added benefit is that they are much more compact than the JSON, even gzipped, and perform on an order of magnitude better in query.</p>",
        "id": 153951389,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1523210965
    },
    {
        "content": "<p>aren't bundles unnecessary? They're just a packaging mechanism. Wouldn't you be better off stripping that out?</p>",
        "id": 153951395,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1523242409
    },
    {
        "content": "<p>They are - the value is accessing the original data as-is without creating a \"stripped out\" copy</p>",
        "id": 153951688,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1523395684
    }
]