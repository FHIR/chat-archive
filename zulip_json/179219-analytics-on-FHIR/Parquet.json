[
    {
        "content": "<p>so on the implementers channel, there's some interest from Cerner in defining and encouraging use of a parquet format for FHIR resources. </p>",
        "id": 153885023,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1492549412
    },
    {
        "content": "<p>does anyone have any comment about this?</p>",
        "id": 153885024,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1492549423
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span>  Apache Parquet will be a nice fit for Hadoop and deep analytics I agree with that . I would encourage that but I think involving on the parquet format will be a challenge because many of us are not familiar with the format </p>",
        "id": 153885084,
        "sender_full_name": "Fahim Shariar",
        "timestamp": 1492594584
    },
    {
        "content": "<p>but do your tools support it?</p>",
        "id": 153885098,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1492604156
    },
    {
        "content": "<p>No our tools does not support it . We are trying a different approach ( Ontological ) with Graph database . Currently with Neo4j . We convert the FHIR JSON resource to Graph Nodes and Edges. Currently we are studying FHIR Resource and trying to find possible graph nodes relationship within a single resource which wont have an orphan child node and will be able to connect to other nodes from other resource .</p>",
        "id": 153885604,
        "sender_full_name": "Fahim Shariar",
        "timestamp": 1492899949
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span>  parquet is related to <em>avro</em> and our JSON format (and JSON schema) - if we fix polymorphics &amp; extensions - we'll get clear JSON schema - and avro + parquet for free :)</p>\n<blockquote>\n<p>To convert your JSON, you need to convert the records to Avro in-memory objects and pass those to Parquet, but you don't need to convert a file  to Avro and then to Parquet.<br>\nConversion to Avro objects is already done for you, see Kite's JsonUtil, and is ready to use as a file reader. The conversion method needs an Avro schema, but you can use that same library to infer an Avro schema from JSON data.</p>\n</blockquote>",
        "id": 153885617,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1492927115
    },
    {
        "content": "<p>Here is also apache arrow, protobuf and plenty of other formats-  yaml, edn, message pack - most of them are very sensitive  to FHIR JSON design</p>",
        "id": 153885618,
        "sender_full_name": "nicola (RIO/SS)",
        "timestamp": 1492927379
    },
    {
        "content": "<p>We are also looking at analytics on FHIR and introducing parquet/avro format would help with that. Tools for data analytics work better with those formats then with pure JSON. As @<strong>nicola (RIO)</strong>  wrote this would probably end up with some fixes applied but it would open a lot of possibilities around using Hadoop based tools.</p>\n<p>A side effect of this step would be also easier upgrade with schema versioning. <a href=\"https://avro.apache.org/docs/1.8.1/spec.html#Schema+Resolution\" target=\"_blank\" title=\"https://avro.apache.org/docs/1.8.1/spec.html#Schema+Resolution\">https://avro.apache.org/docs/1.8.1/spec.html#Schema+Resolution</a></p>",
        "id": 153885695,
        "sender_full_name": "Krzysztof Suchomski",
        "timestamp": 1493022369
    },
    {
        "content": "<p>Hi all,  </p>\n<p>FHIR resources can indeed be serialized with every nested data structure (json, avro, parquet, hive table...postgresql, mysql jsonb datatype, python/R backed application and more). Then each resources are linked with relational links then nosql such couchdb, mongodb looks not to be a good solution to me. </p>\n<p>PARQUET format is AFAIK not intended for real time application because of immutable format (columnar format oriented). Moreover, the way one use parquet (thought apache DRILL or HIVE) is not user friendly to query nested structures. <br>\nAVRO is designed (row format oriented) to ingest real time data. </p>\n<p>Beyong those technical aspects can one discuss those step in order to deliver a \"FHIR ready database\": <br>\n1) define profiles and tel how you implement resources (is spark furore the only existing tool?)<br>\n2) implement resources with data<br>\n3) implement resources with metadata (say data elements, coding systems...)</p>\n<p>In theory this would allow data-scientists to prepare programs using only defined profiles ?</p>",
        "id": 153885885,
        "sender_full_name": "natus",
        "timestamp": 1493109355
    },
    {
        "content": "<p>Hi,<br>\nI have found more details about apache drill, parquet and FHIR analytics.<br>\n<a href=\"http://amia.analystseim.com/\" target=\"_blank\" title=\"http://amia.analystseim.com/\">http://amia.analystseim.com/</a><br>\n<a href=\"http://wiki.hl7.org/index.php?title=201705_Data_Analytics\" target=\"_blank\" title=\"http://wiki.hl7.org/index.php?title=201705_Data_Analytics\">http://wiki.hl7.org/index.php?title=201705_Data_Analytics</a></p>",
        "id": 153885905,
        "sender_full_name": "natus",
        "timestamp": 1493115165
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"193729\">@natus</span> FYI Forge is the FHIR profile editor by Furore, my company (spark is the name of our old and obsolete DSTU2 FHIR server, which is superseded by our new Vonk server). David Hay develops ClinFhir, an alternative profiling tool designed specifically for clinicians. Trifolia by Lantana is another online tool that supports FHIR profiling. And the MDHT tool for UML-based modelling also supports FHIR.</p>",
        "id": 153885907,
        "sender_full_name": "Michel Rutten",
        "timestamp": 1493118470
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191336\">@Michel Rutten</span>  Does Vonk have an End Point yet?</p>",
        "id": 153885939,
        "sender_full_name": "Michael Osborne",
        "timestamp": 1493130378
    },
    {
        "content": "<p>\"but do your tools support it?\"<br>\nThe Meteor/Node ecosystem apparently has a decently robust node-parquet package that we could pull into Meteor on FHIR.   There's also a mongo/hadoop adapter on the Hadoop side of things.  Wouldn't be our first choice of formats, but we can roll with parquet.    </p>\n<p><a href=\"https://www.npmjs.com/package/node-parquet\" target=\"_blank\" title=\"https://www.npmjs.com/package/node-parquet\">https://www.npmjs.com/package/node-parquet</a><br>\n<a href=\"https://github.com/mongodb/mongo-hadoop\" target=\"_blank\" title=\"https://github.com/mongodb/mongo-hadoop\">https://github.com/mongodb/mongo-hadoop</a></p>",
        "id": 153885949,
        "sender_full_name": "Abbie Watson",
        "timestamp": 1493131434
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"192047\">@Michael Osborne</span> certainly, the official Vonk STU3 release is scheduled for tomorrow. We will announce the new endpoint on Zulip. Stay tuned!</p>",
        "id": 153885957,
        "sender_full_name": "Michel Rutten",
        "timestamp": 1493132547
    },
    {
        "content": "<p>I'll add some context from the initial thread that kicked off this discussion. We (Cerner) are interested in an efficient binary format that can be used to share hundreds of millions of FHIR resources and load them into a variety of analytic tools. As others have pointed out in this thread, there are a number of good options here: things like Protocol Buffers or Avro are certainly up to the task as well. If those (or others) have a deeper integration into other ecosystems and are more easily used, then it's worth looking into.</p>\n<p>I originally floated the idea of Parquet since it uses a columnar structure that can be efficiently and directly served by a large set of scalable analytic tools, requiring no (potential expensive) transformation.  This includes Apache projects like Spark, Impala, and Presto and several scalable database systems like Amazon Athena, Google BigQuery, Microsoft PolyBase, and HP Vertica. </p>\n<p>Of course, other formats could be converted to Parquet (there already exists good tooling for bidirectional Avro/Parquet integration), so we don't necessarily need to conflate efficient data serving with efficient data sharing.</p>",
        "id": 153886024,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1493155110
    },
    {
        "content": "<p>Avro is better documented than Parquet. </p>",
        "id": 153886025,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1493155160
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> Agreed. Avro is also more mature and evolved to be more user-facing, whereas Parquet evolved as a file format.Avro could make a great deal of sense for those reasons, and I think we'd be happy with that if it seems like a better fit to the community.</p>",
        "id": 153886026,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1493155489
    },
    {
        "content": "<p>Typically, I'd prefer a file format but I just can't find any documentation about Parquet, and nothing tells me how to handle things like schema modularity</p>",
        "id": 153886031,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1493156222
    },
    {
        "content": "<p>There is also ORC columnar format.  These days ORC is a good challenger. ORC is  highly compatible with HIVE, and presto. I am supposed to give it a try soon. <a href=\"https://orc.apache.org/\" target=\"_blank\" title=\"https://orc.apache.org/\">https://orc.apache.org/</a> Moreover, HIVE offers a friendly way to query nested structure see <a href=\"http://thornydev.blogspot.fr/2013/07/querying-json-records-via-hive.html\" target=\"_blank\" title=\"http://thornydev.blogspot.fr/2013/07/querying-json-records-via-hive.html\">http://thornydev.blogspot.fr/2013/07/querying-json-records-via-hive.html</a> .  (see section \"The best option: rcongiu's Hive-JSON SerDe\"). It is really better thant apache drill way of exploding json objects/arrays</p>",
        "id": 153886032,
        "sender_full_name": "natus",
        "timestamp": 1493156435
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> parquet is a binary file format. Here's a link that provides details: <a href=\"http://bigdata.devcodenote.com/2015/04/parquet-file-format.html\" target=\"_blank\" title=\"http://bigdata.devcodenote.com/2015/04/parquet-file-format.html\">http://bigdata.devcodenote.com/2015/04/parquet-file-format.html</a></p>",
        "id": 153887122,
        "sender_full_name": "Peter Bernhardt",
        "timestamp": 1493917957
    },
    {
        "content": "<p>We use it at RH </p>",
        "id": 153887123,
        "sender_full_name": "Peter Bernhardt",
        "timestamp": 1493918019
    },
    {
        "content": "<p>well, that's a summary. It hardly constitutes 'details'</p>",
        "id": 153887142,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1493920479
    },
    {
        "content": "<p>very funny , <span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> </p>",
        "id": 153887144,
        "sender_full_name": "Peter Bernhardt",
        "timestamp": 1493920502
    },
    {
        "content": "<p>I still haven't found enough documentation to describe the actual format so I could produce a file. It's weird...</p>",
        "id": 153887146,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1493920599
    },
    {
        "content": "<p>fair nuff... i'll hunt around some more and let you know if i find anything better</p>",
        "id": 153887147,
        "sender_full_name": "Peter Bernhardt",
        "timestamp": 1493920654
    },
    {
        "content": "<p>thanks</p>",
        "id": 153887148,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1493920745
    },
    {
        "content": "<p>Coming back to this after some offline discussion with <span class=\"user-mention\" data-user-id=\"191360\">@Michelle M Miller</span>, we took a stab at some criteria for what would make sense for bulk sharing, and listed some technology options below along those lines. This isn't comprehensive, and the proposed requirements are certainly subject to discussion and change, but this could help create some structure around our options:</p>\n<p>Required criteria</p>\n<ul>\n<li>Supports billions of records</li>\n<li>Efficiently ETL-able into MPP databases or analytic tools, such as Apache Drill, Impala, Presto, Spark, Hive, or others.</li>\n<li>An efficient encoding</li>\n<li>Works with open source tools with liberal licensing for academic and commercial use.</li>\n</ul>\n<p>Preferred Criteria</p>\n<ul>\n<li>A self-describing file format, so users need not find an external schema or other metadata to ingest the file. This lets such files be shared without * having to pass additional metadata or references with the file to use it.</li>\n<li>The format is splittable, so multiple threads or processes can ingest a file in parallel rather than waiting on a single thread to go top to bottom of a file. (This can make a big difference when ingesting billions of rows.)</li>\n</ul>\n<p>Given these, the technologies break down as follows:</p>\n<p>Meets Required and Preferred:</p>\n<ul>\n<li>Apache Parquet (a columnar format driven by Cloudera and Twitter)</li>\n<li>Apache Avro (a row-wise format with good tooling, probably the most frequently used binary format in the Hadoop ecosystem)</li>\n<li>Apache ORC (a columnar format and Parquet competitor driven by Hortonworks)</li>\n</ul>\n<p>Meets Required but not Preferred:</p>\n<ul>\n<li>Apache Thrift (files are not self-describing)</li>\n<li>Google Protocol Buffers (files are not self-describing)</li>\n<li>Messagepack (the format is not easily splittable)</li>\n<li>HDF5 (the format is not easily splittable)</li>\n</ul>",
        "id": 153887168,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1493926490
    },
    {
        "content": "<p>With Parquet, code is the best documentation. Probably you can have a look at the Thrift data types along with the README in github. Also the below trivial example is a good starting point. <a href=\"https://github.com/apache/parquet-cpp/blob/master/examples/reader-writer.cc\" target=\"_blank\" title=\"https://github.com/apache/parquet-cpp/blob/master/examples/reader-writer.cc\">https://github.com/apache/parquet-cpp/blob/master/examples/reader-writer.cc</a><br>\nOfcourse this uses Parquet-C++ which is not always at par with the Java implementation but is quite good for understanding.</p>\n<p>Another user of Parquet + Spark in previous life</p>",
        "id": 153887223,
        "sender_full_name": "Anand Mohan Tumuluri",
        "timestamp": 1493936227
    },
    {
        "content": "<p>I've never used c++. Code is good documentation for very fine details, but hard as a general documentation format</p>",
        "id": 153887232,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1493955198
    },
    {
        "content": "<p>Ryan, that's good analysis. I agree with required and preferred, except for one additional criteria that should be required from my point of view- * has a healthy and growing community </p>",
        "id": 153887233,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1493955265
    },
    {
        "content": "<p>Hi Ryan/Graham,</p>\n<p>I am working on loading a large number of FHIR resources as Apache Avro objects into Apache Spark for some batch processing. I need to build an Avro schema to facilitate the conversion of JSON into Avro. <a href=\"http://avro4s-ui.landoop.com/\" target=\"_blank\" title=\"http://avro4s-ui.landoop.com/\">http://avro4s-ui.landoop.com/</a> website creates an Avro schema for a specific JSON string but it does not work for a different record of the same resource type. I am looking for a more flexible schema that would accommodate all possible differences in a FHIR resource. Do I need to manually create this schema or is there an easy way to do it? Any pointers in this regard would be greatly appreciated. </p>\n<p>Thanks,<br>\nMythreyi</p>",
        "id": 153895120,
        "sender_full_name": "Mythreyi Solai",
        "timestamp": 1498245584
    },
    {
        "content": "<p>Hi Ryan/Graham,</p>\n<p>I see you have been discussing Apache Avro and Parquet. I have been using FHIR for a while but I am new to Avro and Spark. </p>\n<p>I am working on loading a large number of FHIR resources as Apache Avro objects into Apache Spark for some batch processing. I need to build an Avro schema to facilitate the conversion of JSON into Avro. <a href=\"http://avro4s-ui.landoop.com/\" target=\"_blank\" title=\"http://avro4s-ui.landoop.com/\">http://avro4s-ui.landoop.com/</a> website creates an Avro schema for a specific JSON string but it does not work for a different record of the same resource type. I am looking for a more flexible schema that would accommodate all possible differences in a FHIR resource. Do I need to manually create this schema or is there an easy way to do it? Any pointers in this regard would be greatly appreciated.<br>\n(I replied to your thread about Parquet with my question but then thought I should send you a private message also, just in case)<br>\nThanks,<br>\nMythreyi</p>",
        "id": 153895121,
        "sender_full_name": "Mythreyi Solai",
        "timestamp": 1498245834
    },
    {
        "content": "<p>this is on my todo list but I haven't actually done anything yet</p>",
        "id": 153895141,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1498255581
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> In May, you suggested that you wanted to get a Connectathon track planned in Sept to have a more in depth discussion about bulk data serialization format for analytics.  Do you know if that is still planned or not?  I think <span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span> is available, but I couldn't find the track details to share with him.</p>",
        "id": 153895257,
        "sender_full_name": "Michelle (Moseman) Miller",
        "timestamp": 1498512326
    },
    {
        "content": "<p>There hasn't been a lot of interest, so I haven't actually proposed it</p>",
        "id": 153895258,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1498512533
    },
    {
        "content": "<p>I've tried to solicit some wider interest, but haven't come up with much beyond in principle interest</p>",
        "id": 153895259,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1498512564
    },
    {
        "content": "<p>is it worth pursuing? </p>",
        "id": 153895260,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1498512572
    },
    {
        "content": "<p>we are exploring FHIR and Avro.  Whether it is Avro or Parquet, we would be interested in joining connectathons, etc... We want to put Spark on top of Avro/Parquet on top of FHIR.  <span class=\"user-mention\" data-user-id=\"193824\">@Mythreyi Solai</span> works for our team at Fresenius Medical Care North America.</p>",
        "id": 153895261,
        "sender_full_name": "Radu Craioveanu",
        "timestamp": 1498512770
    },
    {
        "content": "<p>I'm new to this process, but if there is enough interest we'd be happy to work through sharing data along these lines at the connectathan.  We can share test datasets of several FHIR resources in Parquet, Avro, or ORC today, and can adapt to others if there is a better direction. I'm assuming this is the best venue to gauge interest, but if there are others, let me know.</p>",
        "id": 153895264,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1498528894
    },
    {
        "content": "<p>this is the place. I'm interested, though I'm pretty heavily committed to other streams. So we have 2.25 participants now. Traditionally, we want 3+ - but in the end it's up to you guys. If you think that 2.25 is enough, let's do it</p>",
        "id": 153895265,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1498528982
    },
    {
        "content": "<p>I've been working with the <a href=\"http://Mitre.org\" target=\"_blank\" title=\"http://Mitre.org\">Mitre.org</a> Synthentic Mass data with FHIR format in bulk in a JSONL format with Spark SQL - it handles the nested json objects one per line. I'm taking that data, creating the flattened dataframes from it and persisting as Parquet format. works well. </p>",
        "id": 153895268,
        "sender_full_name": "David Taylor",
        "timestamp": 1498529481
    },
    {
        "content": "<p>I don't know if that's an expression of interest in the connectathon stream?</p>",
        "id": 153895274,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1498544860
    },
    {
        "content": "<p>I had found several people with interest in the Biopharma community, but nobody who can get it together in time for San Diego.  More likely for New Orleans.  But will pass this on if something gets going.</p>",
        "id": 153895328,
        "sender_full_name": "Wayne Kubick",
        "timestamp": 1498595530
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"193657\">@Ryan Brush</span> - there's lots of interest in using NDJson for bulk data. It doesn't meet some of the requirements that we talked about a few months ago, but it's like 100x easier to produce. Have you got any thoughts on this?</p>",
        "id": 153908800,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1505503748
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191351\">@Chris Grenz</span> you talked about converting ndjson to parquet - what library would you recommend for doing this from the command line on windows?</p>",
        "id": 153908831,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1505520600
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span>,  I can appreciate the tradeoffs between efficiency and simplicity of implementation for NDJson, particularly if the workload involves relatively infrequent extracts where an N-times performance penalty isn't important. It would be interesting to know what N is in this example to make an informed decision here though. I might put together some simple benchmarks when time allows to give us an idea.  Also, would we keep the door open for content negotiating a more efficient format if the client and server offer them?</p>",
        "id": 153908930,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1505759143
    },
    {
        "content": "<p>We'd definitely want to keep the door open for content-type negotiation; the thought is that it's helpful to have a common \"must support\" baseline format. Our in-person discussions assumed O(100x) load-time difference between NDJson-vs-Parket.</p>",
        "id": 153908931,
        "sender_full_name": "Josh Mandel",
        "timestamp": 1505759565
    },
    {
        "content": "<p>This sounds reasonable to me. Lots of details to work out, and I'll want to better understand the impact on our workloads to determine how quickly we might pursue other formats, but as a broad direction I think this works.</p>",
        "id": 153908935,
        "sender_full_name": "Ryan Brush",
        "timestamp": 1505760755
    },
    {
        "content": "<p>yes. mandating support for nd-json wouldn't preclude support for parquet by content-negotiation</p>",
        "id": 153908939,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1505768813
    },
    {
        "content": "<p>I've been using Drill directly</p>",
        "id": 153909327,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1505916397
    },
    {
        "content": "<p>Impala also supports parquet creation from anything it can read.</p>",
        "id": 153909328,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1505916816
    },
    {
        "content": "<p>The tricky part is retaining the JSON structure - many tools will convert csv or a flat format. Drill does well with the nested.</p>",
        "id": 153909329,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1505916928
    },
    {
        "content": "<p>Spark supports both nd-JSON and Parquet, so that's an option: <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files\" target=\"_blank\" title=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files\">http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files</a></p>",
        "id": 153909330,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1505917222
    },
    {
        "content": "<p>how would I do it using Drill directly?</p>",
        "id": 153909468,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1505944562
    },
    {
        "content": "<p>1. Install Drill (download and unpack tarball) and run at command line: <a href=\"https://drill.apache.org/docs/installing-drill-on-linux-and-mac-os-x/\" target=\"_blank\" title=\"https://drill.apache.org/docs/installing-drill-on-linux-and-mac-os-x/\">https://drill.apache.org/docs/installing-drill-on-linux-and-mac-os-x/</a><br>\n2. Open browser to <a href=\"http://localhost:8047/storage\" target=\"_blank\" title=\"http://localhost:8047/storage\">http://localhost:8047/storage</a><br>\n3. Update dfs plugin - copy one of the workspaces and update the location as you see fit, set writable:true<br>\n4. Go to query and run something like:  <code>CREATE TABLE dfs.mywkspc.mypqfiles AS SELECT * FROM dfs.mywkspc.`some.file.json`</code></p>",
        "id": 153909800,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1506028232
    },
    {
        "content": "<p>If you <em>really</em> want to do this from command line, once the workspace is configured you can run commands at the command line with the sqlline executable and the --run option:  <a href=\"https://community.mapr.com/docs/DOC-1570\" target=\"_blank\" title=\"https://community.mapr.com/docs/DOC-1570\">https://community.mapr.com/docs/DOC-1570</a></p>",
        "id": 153909802,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1506028402
    },
    {
        "content": "<p>Note this silly issue: <a href=\"https://issues.apache.org/jira/browse/DRILL-5698\" target=\"_blank\" title=\"https://issues.apache.org/jira/browse/DRILL-5698\">https://issues.apache.org/jira/browse/DRILL-5698</a><br>\nWorkaround is comment out lines 396-399 in the bin/drill-config.sh file</p>",
        "id": 153909809,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1506030544
    },
    {
        "content": "<p>yay for open source</p>",
        "id": 153909810,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1506030559
    },
    {
        "content": "<p>Deep dive on Parquet for the nerds:  <a href=\"https://www.youtube.com/watch?v=MZNjmfx4LMc\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=MZNjmfx4LMc\">https://www.youtube.com/watch?v=MZNjmfx4LMc</a></p>\n<div class=\"youtube-video message_inline_image\"><a data-id=\"MZNjmfx4LMc\" href=\"https://www.youtube.com/watch?v=MZNjmfx4LMc\" target=\"_blank\" title=\"https://www.youtube.com/watch?v=MZNjmfx4LMc\"><img src=\"https://i.ytimg.com/vi/MZNjmfx4LMc/default.jpg\"></a></div>",
        "id": 153909996,
        "sender_full_name": "Chris Grenz",
        "timestamp": 1506116608
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"197632\">@Gidon Gershinsky</span> and his team have been doing some work with FHIR in Parquet and recently shared this in the bulk export channel:  <a href=\"#narrow/stream/179250-bulk-data/topic/Parquet.20Bulk.20Data.20format/near/186183104\" title=\"#narrow/stream/179250-bulk-data/topic/Parquet.20Bulk.20Data.20format/near/186183104\">https://chat.fhir.org/#narrow/stream/179250-bulk-data/topic/Parquet.20Bulk.20Data.20format/near/186183104</a><br>\nThought it might be of interest for folks in this analytics stream as well.</p>",
        "id": 186187743,
        "sender_full_name": "Lee Surprenant",
        "timestamp": 1579616976
    }
]