[
    {
        "content": "<p>is it possible to import/upload the content of a CodeSystem just like using some other formats such as obo and owl?  For example, import the basic description of the CodeSystem, all the codes and names of its concepts, all the synonyms, all the properties/relationships in each step.  Or group these different kinds of contents as separate blocks/sections in a single import file.  In such a way, there would be no need to construct a complete hierarchy/tree to be imported in a single step/batch as the structure of the CodeSystem resource shows.</p>",
        "id": 211786675,
        "sender_full_name": "Lin Zhang",
        "timestamp": 1601483483
    },
    {
        "content": "<p>I would say that is entirely up to the server.  Each server (HAPI, Graham's, Ontoserver, etc.) has mechanism(s) for loading terminologies (particularly the large standard ones).  With some servers the smaller terminologies can often be loaded by POSTing a CodeSystem resource containing the content.</p>",
        "id": 211828753,
        "sender_full_name": "Rob Hausam",
        "timestamp": 1601500840
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191405\">@Rob Hausam</span> Largely, directly based on SQL scripts for the physical db?  In other words, the developer of the terminology-specific import has to be very familiar with the db structure of the server, right?</p>",
        "id": 211855381,
        "sender_full_name": "Lin Zhang",
        "timestamp": 1601510901
    },
    {
        "content": "<p>Yes, that is correct, Lin.  It's not necessarily always SQL based, but it does require familiarity with the underlying db structures.  So far we don't have anything like a standard terminology bulk load API in FHIR (although it would be possible to do quite a bit with ConceptMap itself, depending on what the server can support).</p>",
        "id": 211856940,
        "sender_full_name": "Rob Hausam",
        "timestamp": 1601512368
    },
    {
        "content": "<p>How large is a Terminology resource to be called so \"large\" that is has to be  imported in a bulk way?</p>",
        "id": 211858009,
        "sender_full_name": "Lin Zhang",
        "timestamp": 1601513434
    },
    {
        "content": "<p>There isn't really a fixed answer to that.  It depends primarily on the capabilities and capacity of the particular server.  Making a (very) rough stab at it, I would say that many servers probably could fairly readily handle a CodeSystem resource with up to something like maybe 10,000 codes.  POSTing a CodeSystem resource to a FHIR endpoint on a server, though, doesn't necessarily guarantee that the server will then actually support that code system in subsequent terminology service operations (e.g. $expand, $lookup, $validate-code, terminology-based search, etc.) - but depending on the specific server it might.  But for code systems of &gt; 10,000 codes, that is probably less likely, and I think that any server (that I know of) would use a bulk load process of some kind to support code systems with nearly 100,000 or more codes - including the standard large ones like SNOMED CT and LOINC.</p>",
        "id": 211860188,
        "sender_full_name": "Rob Hausam",
        "timestamp": 1601515944
    },
    {
        "content": "<p>defining that many codes is a really large amount of work. There's just not that many code systems that large</p>",
        "id": 211860443,
        "sender_full_name": "Grahame Grieve",
        "timestamp": 1601516351
    },
    {
        "content": "<p>Granular and incremental APIs for importing might help split the bulk job into smaller ones.</p>",
        "id": 211860753,
        "sender_full_name": "Lin Zhang",
        "timestamp": 1601516712
    },
    {
        "content": "<p><a href=\"https://github.com/aehrc/fhir-owl\">https://github.com/aehrc/fhir-owl</a> is our mechanism for dealing with OWL.  This classifies and then generates an FHIR CodeSystem from the result.  It's how we load in things like HPO</p>",
        "id": 211999539,
        "sender_full_name": "Michael Lawley",
        "timestamp": 1601607270
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191343\">@Michael Lawley</span> Does FHIR ontologically support OWL individuals?</p>",
        "id": 212061272,
        "sender_full_name": "Lin Zhang",
        "timestamp": 1601637551
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"191316\">@Grahame Grieve</span> <span class=\"user-mention\" data-user-id=\"192267\">@Eric Prud'hommeaux</span></p>",
        "id": 212071560,
        "sender_full_name": "Lloyd McKenzie",
        "timestamp": 1601644107
    },
    {
        "content": "<p>That depends on what you want OWL to do with the individual. If you want OWL to perform some logic to e.g. stratify patients by some SNOMED description, sure (see <a href=\"https://github.com/BD2KOnFHIR/BLENDINGFHIRandRDF\">https://github.com/BD2KOnFHIR/BLENDINGFHIRandRDF</a>). If you want OWL to e.g. infer that two Patient records are the same record, I guess you could make Patient.identifier an  inverse function property. I wouldn't advise it, though, as they are probably not the same record, but instead two records about the same person, and I'm not sure how you'd profit from that inference (maybe to clean some data, but SPARQL query would probably be more direct).</p>\n<p>In net, I think the useful stuff is covered, as described in Harold's BLENDINGFHIRandRDF repo and presentations.</p>",
        "id": 212075297,
        "sender_full_name": "Eric Prud'hommeaux",
        "timestamp": 1601645991
    },
    {
        "content": "<p>@<strong>LIoyd McKenzie</strong>@<strong>Eric Prud'hommeaux|192267</strong>  Interesting.  Thanks.</p>",
        "id": 212088414,
        "sender_full_name": "Lin Zhang",
        "timestamp": 1601651882
    }
]