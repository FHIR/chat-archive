[
    {
        "content": "<p>I'm creating bulk fake FHIR data with FSH and Sushi using a simple template. The performance data is below. Each FSH file creates 8 FHIR resources. The IG is <a href=\"https://github.com/WorldHealthOrganization/ddcc\">https://github.com/WorldHealthOrganization/ddcc</a>.</p>\n<p>The test platform is a recent Ryzen 8-core desktop with 32GB RAM on Ubuntu 20.04 with Sushi 2.0.1. Sushi only pegs one CPU core when doing processing. At some point with 4k+ FSH files, it hangs in an overnight run. If you're curious about how to reproduce, you'll get a JS heap error with the default heap size and it needs to be increased, e.g. export NODE_OPTIONS=--max_old_space_size=8192 </p>\n<p>I'm exploring this further and if others are interested or have performance suggestions, please do share. And, yes, something like Synthea is more the correct tool, but I really like Sushi because it supports profiled instances.</p>\n<p>Num FSH files: 1, Elapsed time: 0:08.33, CPU: 25%, Max RAM (KB): 184236<br>\nNum FSH files: 10, Elapsed time: 0:08.98, CPU: 52%, Max RAM (KB): 200344<br>\nNum FSH files: 50, Elapsed time: 0:17.09, CPU: 88%, Max RAM (KB): 248816<br>\nNum FSH files: 100, Elapsed time: 0:32.92, CPU: 87%, Max RAM (KB): 364220<br>\nNum FSH files: 250, Elapsed time: 1:17.96, CPU: 100%, Max RAM (KB): 316440<br>\nNum FSH files: 500, Elapsed time: 3:04.59, CPU: 102%, Max RAM (KB): 668364<br>\nNum FSH files: 1000, Elapsed time: 8:45.82, CPU: 103%, Max RAM (KB): 686640<br>\nNum FSH files: 1500, Elapsed time: 20:51.90, CPU: 105%, Max RAM (KB): 2112520<br>\nNum FSH files: 2000, Elapsed time: 36:20.11, CPU: 104%, Max RAM (KB): 2236724<br>\nNum FSH files: 2500, Elapsed time: 55:26.88, CPU: 104%, Max RAM (KB): 2239712</p>",
        "id": 256867040,
        "sender_full_name": "Richard Stanley",
        "timestamp": 1633782958
    },
    {
        "content": "<p>Hi <span class=\"user-mention\" data-user-id=\"194192\">@Richard Stanley</span>.  SUSHI does sometimes struggle with performance when generating instances.  <span class=\"user-mention\" data-user-id=\"239822\">@Nick Freiter</span> recently implemented a small fix that <em>may</em> help.  Have you tried it again w/ SUSHI 2.1.1?  I'm not sure if it will make a big difference or not (the fix is very targeted to particular uses), but it's worth trying!</p>\n<p>That said, SUSHI is a Node app, which (by default) is single-threaded (hence the use of one core).  It also stores all the incoming files and outgoing results in memory.  Our target use case was IGs -- and in that use case, this approach seemed appropriate (although the occasional Saner IG does stress the limits).  Using SUSHI for bulk data creation would likely require some alternate approaches (possible using \"workers\" and/or intermediate writes, etc.).  If there's anyone in the community who wants to explore that, that would be super cool!  For the time being, you might consider splitting your 4000 SUSHI files into multiple projects and then spinning up multiple SUSHI processes to handle them.  This would allow you to utilize more cores.</p>",
        "id": 257276936,
        "sender_full_name": "Chris Moesel",
        "timestamp": 1634075189
    },
    {
        "content": "<p>Thanks much <span class=\"user-mention\" data-user-id=\"191469\">@Chris Moesel</span> This makes a great deal of sense and thanks for the workaround. I had not thought of just spinning up other instances.</p>",
        "id": 257333589,
        "sender_full_name": "Richard Stanley",
        "timestamp": 1634116004
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"194192\">Richard Stanley</span> has marked this topic as resolved.</p>",
        "id": 257358328,
        "sender_full_name": "Notification Bot",
        "timestamp": 1634129029
    }
]